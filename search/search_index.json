{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to specsuite!","text":""},{"location":"#welcome-to-specsuite","title":"Welcome to specsuite!\u00b6","text":"<p><code>specsuite</code> is a set of tools capable of loading, processing, and calibrating your spectroscopic data! As of now, it is most useful for point-source, long-slit spectrographs, but we are working on adding functionality for other types of objects and instruments! Some of <code>specsuite</code>'s tools include...</p> <ul> <li>Loading FITS data into arrays</li> <li>Applying a flatfield correction</li> <li>Modeling how light is warped onto your detector</li> <li>Extracting sky backgrounds using \"warp models\"</li> <li>Interactively creating a wavelength calibration</li> </ul> <p>...and more! We designed <code>specsuite</code> to be as friendly to new users while still offering robust reduction tools. Nearly every function has an optional \"debug\" argument that allows you to visually inspect the results of each step of our pipeline. Additionally, each function is designed to be as modular as possible. If you only like a small subset of our tools, you can intersperse them with your own!</p>"},{"location":"background_extraction/","title":"Background Subtraction","text":"<p>The target of an observation is rarely the only object contributing light to a given exposure. Background contamination (which, for ground-based telescopes, is largely caused by Earth's atmosphere) can contribute its own wavelength-dependent features. Without careful correction, these erroneous features could show up in our reduced spectra, making it difficult to discern the chromatic properties of our target.</p> <p>In this section, we apply <code>specsuite</code>'s warp models to produce a high-quality estimate of this background contamination across the entire detector.</p> In\u00a0[1]: hide_code_block Copied! <pre>import specsuite as ss\n\nDATA_REGION = (700, 800)\nCAL_PATH = \"../data/KOSMOS/calibrations\"\nDATA_PATH = \"../data/KOSMOS/target\"\nINSTRUMENT = \"kosmos\"\n\n# Loads calibration exposures\nbias = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"bias\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT,\n)\narclamp = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"neon\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT,\n) - bias\n\n# Loads science exposures\nscience = ss.collect_images_array(\n    path = DATA_PATH,\n    tag = \"toi3884\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT,\n) - bias\n\n# Generates a decent warp model for our sample KOSMOS data\nlocs, _ = ss.find_cal_lines(arclamp, std_variation=200)\nwarp_model = ss.generate_warp_model(arclamp, locs)\n</pre> import specsuite as ss  DATA_REGION = (700, 800) CAL_PATH = \"../data/KOSMOS/calibrations\" DATA_PATH = \"../data/KOSMOS/target\" INSTRUMENT = \"kosmos\"  # Loads calibration exposures bias = ss.average_matching_files(     path = CAL_PATH,     tag = \"bias\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT, ) arclamp = ss.average_matching_files(     path = CAL_PATH,     tag = \"neon\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT, ) - bias  # Loads science exposures science = ss.collect_images_array(     path = DATA_PATH,     tag = \"toi3884\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT, ) - bias  # Generates a decent warp model for our sample KOSMOS data locs, _ = ss.find_cal_lines(arclamp, std_variation=200) warp_model = ss.generate_warp_model(arclamp, locs) In\u00a0[2]: Copied! <pre>KWARGS = {\"norm\": \"log\", \"vmin\": 1e2, \"vmax\": 4e3}\n\nss.plot_image(science[0], **KWARGS)\n</pre> KWARGS = {\"norm\": \"log\", \"vmin\": 1e2, \"vmax\": 4e3}  ss.plot_image(science[0], **KWARGS) <p>We can see that the trace spans the cross-dispersion axis from ~40-80 pixels. These rows should be masked out during our background extraction. Once your warp model has been generated, a series of background exposures can be extracted by calling...</p> In\u00a0[3]: Copied! <pre>MASK_REGION = (40, 80)\n\nbackground = ss.extract_background(\n    images = science,\n    warp_model = warp_model,\n    mask_region = MASK_REGION,\n    debug = True,\n)\n</pre> MASK_REGION = (40, 80)  background = ss.extract_background(     images = science,     warp_model = warp_model,     mask_region = MASK_REGION,     debug = True, ) <p>Note that the colorbars of all three plots are on a log scale (values less than 0 show up as white). The fact that the non-illuminated regions of our image look like random static is a sign that our extraction and correction were accurate! If you wanted to verify these results using some additional metrics, <code>extract_background()</code> has some optional returns that may be useful.</p> In\u00a0[4]: Copied! <pre># Re-runs background extraction and collects additional returns\nreturned_data = ss.extract_background(\n    images = science,\n    warp_model = warp_model,\n    mask_region = MASK_REGION,\n    return_spectrum = True,\n)\n\n# Unpacks returned data into separate arrays\n_, background_effpix, background_flux, effpix_map = returned_data\n</pre> # Re-runs background extraction and collects additional returns returned_data = ss.extract_background(     images = science,     warp_model = warp_model,     mask_region = MASK_REGION,     return_spectrum = True, )  # Unpacks returned data into separate arrays _, background_effpix, background_flux, effpix_map = returned_data <p>The first of these new returns we will discuss is called the \"effective pixel map\". Assuming that light is warped as it falls onto camera, a given wavelength may not fall onto the same dispersion axis location. Using the warp models generated in the last step, we can approximate how a wavelength's dispersion-axis location changes along the cross-dispersion axis. So, if two pixels in the \"effective pixel map\" have the same value, that means that the same wavelength falls onto those pixels. Let's take a look at an example to illustrate this concept...</p> In\u00a0[5]: Copied! <pre># Plots a small region of the effective pixel map\nss.plot_image(\n    effpix_map,\n    cbar_label = \"Effective Pixel Location\",\n    xlim = (20, 30),\n    vmin = 20,\n    vmax = 30,\n)\n</pre> # Plots a small region of the effective pixel map ss.plot_image(     effpix_map,     cbar_label = \"Effective Pixel Location\",     xlim = (20, 30),     vmin = 20,     vmax = 30, ) <p>In this zoomed-in view, we can see a slightly curved gradient. Let (x, y) represent the dispersion and cross-dispersion locations in the above image. Since, the color at (26, 0) is identical to the color at (23, 100), the same wavelength falls at both of those locations.</p> <p>The effective pixel map is calculated directly from the provieed warp models. If you are seeing weird features in your extracted background, that is a good sign that something went wrong in your warp model generation.</p> <p>Note</p> <p>         The white bars present in the effective pixel map represent the region we masked out during the background extraction (see the MASK_REGION variable). Each of these pixels has a NaN value to help us filter them out during the extraction process.     </p> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\n\n# Plots the supersampled background spectra\nplt.scatter(\n    returned_data[1],\n    returned_data[2][0],\n    color = \"black\",\n    alpha = 0.4,\n    s = 2,\n)\n\n# Formats the labels and scaling\nplt.xlabel(\"Effective Dispersion-Axis Location [Pixels]\")\nplt.ylabel(\"Effective Flux [Counts]\")\nplt.yscale(\"log\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  # Plots the supersampled background spectra plt.scatter(     returned_data[1],     returned_data[2][0],     color = \"black\",     alpha = 0.4,     s = 2, )  # Formats the labels and scaling plt.xlabel(\"Effective Dispersion-Axis Location [Pixels]\") plt.ylabel(\"Effective Flux [Counts]\") plt.yscale(\"log\") plt.show() <p>This SED is calculated for each individual exposure. The final, slightly complicated step in the background extraction involves taking the median of a subset of the above SED for every pixel in an image. Let (L, R) be the effective pixel locations of a given pixel's left and right edges. Then...</p> <p>\\begin{equation*}     \\textbf{S}(x, y) = \\text{med}\\left[ f(L &lt; x &lt; R) \\right] \\end{equation*}</p> <p>...represents the extracted background contribution. By using a median over a super-sampled SED, we mitigate the impact of cosmic rays, statistical noise, and other weird features during our extraction.</p> <p>Warning</p> <p>         This process is fairly computationally-expensive. For the example KOSMOS data used above, this operation needs to be performed for every pixel of every exposure (~200,000 calculations per image)! We have vectorized many of these operations to make them as efficient as possible, but it still may take several minutes to process your exposures. The extraction also attempts to split up the computations over several cores, but this is only useful for computers with several cores capable of parallel processing.     </p> In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\n\n# Re-runs background extraction and collects additional returns\nreturned_data = ss.extract_background(\n    images = science,\n    warp_model = warp_model,\n    return_spectrum = True,\n)\n\n# Plots the supersampled background spectra\nplt.scatter(\n    returned_data[1],\n    returned_data[2][0],\n    color = \"black\",\n    alpha = 0.4,\n    s = 2,\n)\n\n# Formats the labels and scaling\nplt.xlabel(\"Effective Dispersion-Axis Location [Pixels]\")\nplt.ylabel(\"Effective Flux [Counts]\")\nplt.yscale(\"log\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  # Re-runs background extraction and collects additional returns returned_data = ss.extract_background(     images = science,     warp_model = warp_model,     return_spectrum = True, )  # Plots the supersampled background spectra plt.scatter(     returned_data[1],     returned_data[2][0],     color = \"black\",     alpha = 0.4,     s = 2, )  # Formats the labels and scaling plt.xlabel(\"Effective Dispersion-Axis Location [Pixels]\") plt.ylabel(\"Effective Flux [Counts]\") plt.yscale(\"log\") plt.show() <p>We can see that the trace shows up as odd features in our super-sampled background spectra. These features can interfere with the accuracy of your extracted background. If you are unsure whether you have masked out the right region of your image, check to see if your supersampled background spectra contains these features.</p> In\u00a0[8]: Copied! <pre># THe 'guess' is set to small, dim portions of exposure\nbad_warp_model = ss.generate_warp_model(\n    image = arclamp,\n    guess = [250, 260, 270],\n    tolerance = 2,\n)\n\nbackground = ss.extract_background(\n    images = science,\n    warp_model = bad_warp_model,\n    mask_region = (50, 90),\n)\n\nss.plot_image(background[0], norm = 'log')\n</pre> # THe 'guess' is set to small, dim portions of exposure bad_warp_model = ss.generate_warp_model(     image = arclamp,     guess = [250, 260, 270],     tolerance = 2, )  background = ss.extract_background(     images = science,     warp_model = bad_warp_model,     mask_region = (50, 90), )  ss.plot_image(background[0], norm = 'log') <p>If you encounter this error, please revist the code used to generate your warp model. You may need to adjust some of the optional parameters for your data.</p> In\u00a0[9]: Copied! <pre>import numpy as np\n\n# Generates a fake flatfield with a strong gradient\ngradient = np.linspace(0.5, 1.5, arclamp.shape[0])[:, None]\nbad_flatfield = gradient * np.ones(arclamp.shape[1])\nbad_science = science*bad_flatfield\n\n# Performs a background extraction\nbackground = ss.extract_background(\n    images = bad_science,\n    warp_model = warp_model,\n    mask_region = (50, 90),\n)\n\n# Plots the first 'bad' background correction\nss.plot_image(\n    (bad_science-background)[0],\n    title = \"Resulting Correction\",\n    norm = 'log',\n)\n</pre> import numpy as np  # Generates a fake flatfield with a strong gradient gradient = np.linspace(0.5, 1.5, arclamp.shape[0])[:, None] bad_flatfield = gradient * np.ones(arclamp.shape[1]) bad_science = science*bad_flatfield  # Performs a background extraction background = ss.extract_background(     images = bad_science,     warp_model = warp_model,     mask_region = (50, 90), )  # Plots the first 'bad' background correction ss.plot_image(     (bad_science-background)[0],     title = \"Resulting Correction\",     norm = 'log', ) <p>Ideally, all pixels that are not illuminated by the signal trace should look like random static. If you see clear structure over these non-illuminated pixels, you should double-check that you have performed a flatfield calibration on your exposures.</p>"},{"location":"background_extraction/#basic-usage","title":"Basic Usage\u00b6","text":""},{"location":"background_extraction/#background-extraction","title":"Background Extraction\u00b6","text":"<p>Once a warp model has been generated, it is fairly trivial to perform a background extraction. The only additional piece of information we need is which rows should be masked to prevent the signal trace from throwing off our estimate of the background emission. For example, take a look at the following exposure...</p>"},{"location":"background_extraction/#effective-pixel-map","title":"Effective Pixel Map\u00b6","text":"<p>To understand how the above function works, it can be helpful to look at some option returns for <code>extract_background()</code> using the <code>return_spectrum = True</code>...</p>"},{"location":"background_extraction/#extracted-background-sed","title":"Extracted Background SED\u00b6","text":"<p>Once the effective pixel map has been generated, <code>extract_background()</code> attempts to estimate how strong the background emissions are as a function of effective pixel location. It does this by scanning over each unmasked row, extracting the brightness at each pixel in the row, and adding each pixel to a spectra with (effective pixel location, counts) being the point's coordinates. Two of the optional returns allow us to visualize this super-sampled background emission spectra...</p>"},{"location":"background_extraction/#common-errors","title":"Common Errors\u00b6","text":""},{"location":"background_extraction/#bad-masking","title":"Bad Masking\u00b6","text":"<p>While providing a MASK_REGION is not necessary for <code>extract_background()</code> to run, we highly recommend doing so. Let's take a look at an example where we did not mask out the signal trace...</p>"},{"location":"background_extraction/#bad-warp-models","title":"Bad Warp Models\u00b6","text":"<p>If the <code>warp_model</code> produced by <code>generate_warp_model()</code> is poor, the extracted background will often look weird. There are plenty of situations that could lead to poor model generation, but it usually comes down to a bad choice of <code>guess</code> or <code>tolerance</code>. For example, if we only look at a small region of data and shrink our tolerance, we get the following results...</p>"},{"location":"background_extraction/#non-uniform-background","title":"Non-Uniform Background\u00b6","text":"<p>This algorithm assumes that all background emissions are approximately uniform along to the cross-dispersion direction. If your background is spectrally non-uniform, then the resulting background-subtraction can produce large residuals...</p>"},{"location":"contact/","title":"Contact Us","text":"<p>If you find an issue with <code>specsuite</code> or our documentation, we would love to hear from you! You can either leave an \"Issue\" on our GitHub repo or email us here.</p>"},{"location":"dewarping/","title":"Model Image Distortion","text":"<p>Once that standard suite of calibrations have been performed, we need to subtract off light contamination from background sources. The trickiest aspect of this calibration is trying to interpolate how bright the background is over the region of pixels that our signal trace spans. If your spectrograph is well-behaved, then all wavelengths will fall at a single location along the dispersion axis. Real spectrographs are rarely this ideal. Often, line emissions may show up as curved lines on the detector due to imperfections in the instrument's optics.</p> <p>Here, we discuss how <code>specsuite</code> models distortions in an image and the methods used to correct them.</p> <p>Note</p> <p>         Each documentation page builds upon tools used in the prior section. To make each page easy to read, the code used to set up each page has been hidden. If you would like to see the code block we are running to initialize each page, you can use the \"Show Setup Code\" button.     </p> In\u00a0[1]: hide_code_block Copied! <pre>import specsuite as ss\n\n# Defines where to look for data\nDATA_REGION = (700, 800)\nCAL_PATH = \"../data/KOSMOS/calibrations\"\nDATA_PATH = \"../data/KOSMOS/target\"\nINSTRUMENT = \"kosmos\"\n\n# Collects calibration exposures\nbias = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"bias\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT\n)\narclamp = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"neon\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT,\n) - bias\n\n# Collects all science exposures\nscience = ss.collect_images_array(\n    DATA_PATH,\n    \"toi3884\",\n    crop_bds=DATA_REGION,\n    instrument = INSTRUMENT,\n) - bias\n</pre> import specsuite as ss  # Defines where to look for data DATA_REGION = (700, 800) CAL_PATH = \"../data/KOSMOS/calibrations\" DATA_PATH = \"../data/KOSMOS/target\" INSTRUMENT = \"kosmos\"  # Collects calibration exposures bias = ss.average_matching_files(     path = CAL_PATH,     tag = \"bias\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT ) arclamp = ss.average_matching_files(     path = CAL_PATH,     tag = \"neon\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT, ) - bias  # Collects all science exposures science = ss.collect_images_array(     DATA_PATH,     \"toi3884\",     crop_bds=DATA_REGION,     instrument = INSTRUMENT, ) - bias In\u00a0[2]: Copied! <pre>line_locations, line_intensities = ss.find_cal_lines(\n    image = arclamp,\n    std_variation = 200,\n    debug = True,\n)\n</pre> line_locations, line_intensities = ss.find_cal_lines(     image = arclamp,     std_variation = 200,     debug = True, ) <p>Note</p> <p>         If multiple adjacent pixels exceed the provided threshold, they will be combined into a single line at the mean of their locations. When the provided threshold is too low, this can result in several distinct lines being lumped into a single, effective location.     </p> In\u00a0[3]: Copied! <pre>models = ss.generate_warp_model(\n    image = arclamp,\n    guess = line_locations,\n    debug = True,\n)\n</pre> models = ss.generate_warp_model(     image = arclamp,     guess = line_locations,     debug = True, ) <p>This large plot zooms in to each line provided in <code>guess</code> and overplots the fitted warp model. As long as the white grid's curvature seems to mirror that of the line emissions, then the warp model was generated correctly!</p> In\u00a0[4]: Copied! <pre># Applies de-warping to arclamp image\nfixed_arc = ss.dewarp_image(\n    image = arclamp,\n    models = models,\n)\n\n# Before correction\nss.plot_image(\n    arclamp,\n    norm = 'log',\n    xlim = (1050, 1070),\n    vmin = 1e3,\n    vmax = 5e4,\n)\n\n# After correction\nss.plot_image(\n    fixed_arc,\n    norm = 'log',\n    xlim = (1050, 1070),\n    vmin = 1e3,\n    vmax = 5e4,\n)\n</pre> # Applies de-warping to arclamp image fixed_arc = ss.dewarp_image(     image = arclamp,     models = models, )  # Before correction ss.plot_image(     arclamp,     norm = 'log',     xlim = (1050, 1070),     vmin = 1e3,     vmax = 5e4, )  # After correction ss.plot_image(     fixed_arc,     norm = 'log',     xlim = (1050, 1070),     vmin = 1e3,     vmax = 5e4, ) <p>This warp model applies to more than just your arclamp image, though! For example, we can use it to straighten out a science exposure...</p> In\u00a0[5]: Copied! <pre># Applies warp model to a single science image\nfixed_science = ss.dewarp_image(science[0], models)\n\n# Plots the 'before' and 'after' sub-images\nss.plot_image(\n    science[0],\n    norm = 'log',\n    xlim = (300, 500),\n    vmin = 1e2,\n    vmax = 1e4,\n)\n\nss.plot_image(\n    fixed_science,\n    norm = 'log',\n    xlim = (300, 500),\n    vmin = 1e2,\n    vmax = 1e4,\n)\n</pre> # Applies warp model to a single science image fixed_science = ss.dewarp_image(science[0], models)  # Plots the 'before' and 'after' sub-images ss.plot_image(     science[0],     norm = 'log',     xlim = (300, 500),     vmin = 1e2,     vmax = 1e4, )  ss.plot_image(     fixed_science,     norm = 'log',     xlim = (300, 500),     vmin = 1e2,     vmax = 1e4, ) <p>Warning</p> <p>             We do not recommend using dewarp_image() for anything other than visual inspection. This is because the coordinate rectification technique produces faint aliasing artifacts that could easily be mistaken for an astronomical signal.         </p> In\u00a0[6]: Copied! <pre>_, _ = ss.find_cal_lines(\n    arclamp,\n    debug = True,\n    std_variation = 1,\n)\n</pre> _, _ = ss.find_cal_lines(     arclamp,     debug = True,     std_variation = 1, ) <p>Notice that several lines from ~300-900 are considered a single feature centered at ~600. If you encounter this error, try increasing the <code>std_variation</code> argument.</p> In\u00a0[7]: Copied! <pre># Generates a bad warp model\nwarp_model = ss.generate_warp_model(\n    image = arclamp,\n    guess = [250, 260, 270],\n    tolerance = 2,\n)\n\n# Attempts to de-warp an image using the bad model\ndewarped_image = ss.dewarp_image(\n    image = arclamp,\n    models = warp_model,\n)\n\nss.plot_image(dewarped_image, norm='log')\n</pre> # Generates a bad warp model warp_model = ss.generate_warp_model(     image = arclamp,     guess = [250, 260, 270],     tolerance = 2, )  # Attempts to de-warp an image using the bad model dewarped_image = ss.dewarp_image(     image = arclamp,     models = warp_model, )  ss.plot_image(dewarped_image, norm='log') <p>...shows what a poor warp model can do to your data. You could also diagnose the issue by looking at <code>generate_warp_model()</code>'s debugging plots...</p> In\u00a0[8]: Copied! <pre>warp_model = ss.generate_warp_model(\n    arclamp,\n    guess = [250, 260, 270],\n    tolerance = 2,\n    debug = True,\n)\n</pre> warp_model = ss.generate_warp_model(     arclamp,     guess = [250, 260, 270],     tolerance = 2,     debug = True, ) <p>If you run into this issue, simply try increasing the <code>tolerance</code> or choose different points for your <code>guess</code> array.</p>"},{"location":"dewarping/#basic-usage","title":"Basic Usage\u00b6","text":""},{"location":"dewarping/#finding-line-emissions","title":"Finding Line Emissions\u00b6","text":"<p>Arclamps produce distinct line emissions depending on which element they are filled with. In order to perform a wavelength calibration, we first need a list of pixel locations that line emissions appear at. <code>specsuite</code> does this by looking for pixels that have a value that exceeds some multiple of the mean-averaged deviation above the median. So, if we wanted to flag all lines with an intensity 200x greater than the median, we could use the following code...</p>"},{"location":"dewarping/#generating-a-warp-model","title":"Generating a \"Warp Model\"\u00b6","text":"<p>Several <code>specsuite</code> functions use a warp model that describes how straight, vertical lines are curved along the dectector. Specifically, if we assume that a line emission takes on the shape of an Nth-order polynomial, then the shape of each line emission at a given dispersion-axis location (x) is...</p> <p>\\begin{equation*}     W(x) = \\sum_{i=0}^N c_i(x) \\cdot x^i \\end{equation*}</p> <p>where each coefficient is a separate Mth-order polynomial (these describe how the line shape changes across the detector). As you can imagine, this can be a difficult model to build. However, using the line emission features contained in an arclamp image, we can automate how we generate warp models!</p> <p>We use <code>scipy.signal</code> to measure the cross-correlation between rows. This gives us a list of \"lags\" between rows that traces out the same curvature as our original signal. By fitting an Nth-order polynomial to these \"lag curves\", we can approximate the shape of spectral lines in that region of the detector. Then, we fit a separate Mth-order polynomial to each fitting consant to model how these line change shape along the dispersion axis.</p> <p>Generally, you just need to provide the arclamp image and the approximate locations of line emissions in that image. Using the line locations from <code>find_cal_lines()</code>...</p>"},{"location":"dewarping/#fixing-warped-images","title":"Fixing Warped Images\u00b6","text":"<p>One potential use for a warp model is to \"de-warp\" an image. Essentially, this tries to re-distribute pixel counts in a way that accounts for the contorted pixel grid (a practice known as coordinate rectification). If the warp model is good, then the curved line emissions in our arclamp image should be straightened out...</p>"},{"location":"dewarping/#common-errors","title":"Common Errors\u00b6","text":""},{"location":"dewarping/#lines-are-lumped-together","title":"Lines are Lumped Together\u00b6","text":"<p>The biggest issue you may run into with <code>find_cal_lines()</code> is when your threshold is set too high or low. If the threshold is so high that no lines are found, it will throw a warning and use 'None' for both returns. However, setting too low of a threshold can produce a sneakier error. Adjacent bright pixels are combined into a single effective line. This means that a low threshold can result in many distinct lines being coalesced into one nonsensical feature...</p>"},{"location":"dewarping/#poor-warp-model","title":"Poor Warp Model\u00b6","text":"<p>If you notice some weird residuals during your data reduction, it is a good idea to double-check the quality of your warp model. The <code>generate_warp_model()</code> function makes a handful of assumptions about your image. If your <code>tolerance</code> is too small or your <code>guess</code> array only covers a small portion of your image, the resulting model will be poor. For example...</p>"},{"location":"docstring_documentation/","title":"Docstrings","text":""},{"location":"docstring_documentation/#_gmos_loader","title":"_GMOS_loader","text":"<p> specsuite.loading._GMOS_loader ( path, file, return_RN = False,): [SOURCE]</p> <p>Description:Controls how to load data from Gemini Observatory's GMOS-N instrument. The resulting output will be oriented such that the x-axis is the dispersion axis (left is blue / right is red) and the y-axis is the cross-dispersion axis. </p> <p>Parameters:path :: strDirectory pointing toward the FITS file you wish to     load. This should not include the name of the file     itself.file :: strName of the FITS file in the specified directory to     load.return_RN :: boolDetermines whether the read noise image should be     returned as an additional return. Defaults to 'False'.</p> <p>Returns:image :: np.ndarrayA 2D array loaded in from the specified FITS file.</p>"},{"location":"docstring_documentation/#_kosmos_loader","title":"_kosmos_loader","text":"<p> specsuite.loading._kosmos_loader ( path, file, clip_overscan = True,): [SOURCE]</p> <p>Description:Controls how to load data from Apache Point Observatory's KOSMOS instrument. The resulting output will be oriented such that the x-axis is the dispersion axis (left is blue / right is red) and the y-axis is the cross-dispersion axis. </p> <p>Parameters:path :: strDirectory pointing toward the FITS file you wish to     load. This should not include the name of the file     itself.file :: strName of the FITS file in the specified directory to     load.clip_overscan :: boolDetermines whether to clip the overscan region of the     detector.</p> <p>Returns:image_data :: np.ndarrayA 2D array loaded in from the specified FITS file.</p>"},{"location":"docstring_documentation/#average_matching_files","title":"average_matching_files","text":"<p> specsuite.loading.average_matching_files ( path, tag, instrument = \"kosmos\", ignore = [], crop_bds = [0, None], mode = \"median\", debug = False, progress = False,): [SOURCE]</p> <p>Description:Extracts images from a user-given path, and finds the average pixel value for every pixel across all images. This defaults to the 'median' average, but can be changed to take the 'mean' average as well. </p> <p>Parameters:path :: strPath to data directory.tag :: strTag to search for in filenames.instrument :: strThe name of the instrument your FITS data was taken from. This     is only used to determine which loading function to use.ignore :: listList of data indexes to ignore in averaging.crop_bds :: listThe region along the cross-dispersion (spatial) axis     to keep (all other rows will be dropped).mode :: strType of average to take of images. Valid inputs     include 'median' and 'mean'.debug :: boolToggles the display of image stats.progress :: boolToggles the progress bar.</p> <p></p>"},{"location":"docstring_documentation/#collect_images_array","title":"collect_images_array","text":"<p> specsuite.loading.collect_images_array ( path, tag, ignore = None, crop_bds = [0, None], instrument = \"kosmos\", clip_overscan = True, debug = False, progress = False,): [SOURCE]</p> <p>Description:Collect a list of images from a user-given path corresponding to a specified tag. Images can be ignore by passing their indexes as an additional argument. </p> <p>Parameters:path :: strPath to data directory containing image     data.tag :: strTag to search for in filenames.ignore :: listList of file indexes to ignore.crop_bds :: listThe region along the cross-dispersion (spatial) axis     to keep (all other rows will be dropped).instrument :: strThe name of the instrument the FITS data was     taken from. This is used to determine which loading     function should be used.clip_overscan :: boolAllows the overscan region to be cropped out of     the returned array.debug :: boolAllows for diagnostic information to be printed.     This includes the names of all files found with     the given 'tag' and whether any of them failed     to load.progress :: boolWhether a progress bar should be displayed.</p> <p>Returns:image_collection :: np.ndarrayAn array of 2D images corresponding to each valid     file found in the provided path.</p>"},{"location":"docstring_documentation/#extract_image","title":"extract_image","text":"<p> specsuite.loading.extract_image ( path, file, instrument,): [SOURCE]</p> <p>Description:Attempts to extract image data from a given FITS file using a method specific to the user-specified instrument. </p> <p>Parameters:path :: strDirectory pointing toward the FITS file you wish to     load. This should not include the name of the file     itself.file :: strName of the FITS file in the specified directory to     load.instrument :: strSpecifies which loading function should be used for     the FITS file. Currently, the only supported instruments     are...         - KOSMOS         - GMOS</p> <p>Returns:image :: np.ndarray | NoneA 2D array containing the image found in the resulting FITS     file. If there is issue with loading the data, a 'None' is     returned.</p>"},{"location":"docstring_documentation/#extract_times","title":"extract_times","text":"<p> specsuite.loading.extract_times ( path, tag, ignore = [], time_lbl = \"DATE-OBS\", ra_lbl = \"RA\", dec_lbl = \"DEC\", lat_lbl = \"LATITUDE\", long_lbl = \"LONGITUD\", time_format = \"isot\", time_scale = \"tai\", loc_units = (Unit(\"hourangle\"), Unit(\"deg\")), loc_frame = \"icrs\",): [SOURCE]</p> <p>Description:Extracts time data from the headers of a set of observations. Assumes that the header has information about the observation time. </p> <p>Parameters:path :: strDirectory pointing toward the FITS file you wish to     load. This should not include the name of the file     itself.tag :: strA sub-string that can help differentiate between     desired and undesired files in a directory. If     an empty string is provided, no files are filtered     out (based on the 'tag' criteria).ignore :: listFilenames to ignore when loading in data. The 'ignore'     filenames must exactly match how they appear in the     file navigator (including .fits extension).time_lbl :: strHeader label for observation time.ra_lbl :: strHeader label for RA of target.dec_lbl :: strHeader label for DEC of target.lat_lbl :: strHeader label for latitude of target.long_lbl :: strHeader label for longitude of target.time_format :: strAstropy Time() format that represents the     time data in the header.time_scale :: strAstropy Time() scale that represents the     time data in the header.loc_units :: tupleAstropy SkyCoord() units that represents     the (RA, DEC) data in the header.loc_frame :: strAstropy SkyCoord() frame that represents     the (RA, DEC) data in the header.</p> <p>Returns:times_bc :: np.ndarrayArray of JD barycentric times that have     been corrected for variations in light     travel time. Has attached astropy units     of days.</p>"},{"location":"docstring_documentation/#filter_files","title":"filter_files","text":"<p> specsuite.loading.filter_files ( files, tag, ignore,): [SOURCE]</p> <p>Description:Filters down a list of filenames if they to not satisfy the following requirements...      1) The file ends with '.fits' extension     2) The provided 'tag' is not in the filename     3) The filename is not given in 'ignore' list </p> <p>Parameters:files :: listSeveral filenames to filter based on the above     criteria.tag :: strA sub-string that can help differentiate between     desired and undesired files in a directory. If     an empty string is provided, no files are filtered     out (based on the 'tag' criteria).ignore :: listFilenames to ignore when loading in data. The 'ignore'     filenames must exactly match how they appear in the     file navigator (including .fits extension).</p> <p>Returns:files :: listAll remaining files once filtering has been performed.</p>"},{"location":"docstring_documentation/#load_metadata","title":"load_metadata","text":"<p> specsuite.loading.load_metadata ( path, tag, ignore = [],): [SOURCE]</p> <p>Description:Loads an dictionary of all data for a collection of FITS files. This metadata comes from the header of the first FITS card. </p> <p>Parameters:path :: strPath to data directory.tag :: strTag to search for in filenames.ignore :: listList of data indexes to ignore.</p> <p>Returns:metadata :: dictDictionary containing the metadata     found for each key in the FITS headers.     Keys-value pairs that are identical     across all exposures are combined into     a single value.</p>"},{"location":"docstring_documentation/#split_chips","title":"split_chips","text":"<p> specsuite.loading.split_chips ( images,): [SOURCE]</p> <p>Description:Attempts to split up a series of 2D images into separate arrays for each \"chip\" that has been combined. This function assumes that \"chip gaps\" are indicated by a column that is entirely comprised of NaN values. </p> <p>Parameters:images :: np.ndarrayA series of images that are comprised of multiple chips     joined by a \"chip gap\" comprised of NaN values.</p> <p>Returns:sub_images :: np.ndarrayA list of images where each entry has N sub-images that     make up each chip that was detected.</p>"},{"location":"docstring_documentation/#combine_within_tolerance","title":"combine_within_tolerance","text":"<p> specsuite.warping.combine_within_tolerance ( values, tolerance,): [SOURCE]</p> <p>Description:Takes a user-given list and combines values that are within a given tolerance. This is helpful for when a dense sample of features is non-ideal and should be combined into a single feature. </p> <p>Parameters:values :: np.ndarrayList of values to be analyzed.tolerance :: floatThe theshold at which two data points should be combined.</p> <p>Returns:combined_values :: listList of values where close points have been averaged and     combined.</p>"},{"location":"docstring_documentation/#dewarp_image","title":"dewarp_image","text":"<p> specsuite.warping.dewarp_image ( image, models, debug = False, progress = False,): [SOURCE]</p> <p>Description:Uses a 'warp model' to undo distortion in a 2D image. This is done by rebinning each row onto a consistent wavelength bin proportional to the overlapping area between the raw image data and a warped set of new pixel locations. </p> <p>Parameters:image :: np.ndarrayImage that is horizontally warped according     to a set of models the user also provides.models :: listList of models that describe how vertical lines     are warped along the image's horizontal axis.debug :: boolEnables diagnostic plots.progress :: boolEnables a progress bar.</p> <p>Returns:unwarped_image :: np.ndarrayA dewarped version of the provided image.</p>"},{"location":"docstring_documentation/#extract_background","title":"extract_background","text":"<p> specsuite.warping.extract_background ( images, warp_model, mask_region = (None, None), return_spectrum = False, progress = False, debug = False,): [SOURCE]</p> <p>Description:Extracts the sky background for a series of exposures where a trace should be masked out. It uses a 'warp model' that describes how straight emission lines are 'bent' on the imaging plane. This function assumes that flatfielding has been applied and that sky emissions are approximately uniform along the spatial axis. </p> <p>Parameters:images :: np.ndarrayA series of exposures to extract backgrounds from.warp_model :: listList of models that describe how vertical lines     are warped along the image's horizontal axis.mask_region :: tupleThe pixel locations (vertical) to mask out during the     background extraction.return_spectrum :: boolWhether or not to return the super-sampled spectra used for     interpolating the sky background. If 'True', three additional     arguments are returned. These are the effective pixel     locations, effective flux, and the 'effpix map'.progress :: boolControls whether or not progress bars are shown.debug :: boolAllows for diagnostic plots to be shown.</p> <p>Returns:background_images :: np.ndarrayA series of images representing the approximated sky background     for each image.supersampled_effpix :: np.ndarrayThe grid of 'effective pixel locations' that the sky emission     spectra was extracted for. This is essentially a flattened     version of the 'effpix_map'.supersampled_spectra :: np.ndarrayThe effective flux found for each of the effective pixel     locations.effpix_map :: np.ndarrayA 2D array with the same size as an individual image. The value     at each pixel represents the 'effective pixel location' for     that given pixel.</p>"},{"location":"docstring_documentation/#find_cal_lines","title":"find_cal_lines","text":"<p> specsuite.warping.find_cal_lines ( image, std_variation = 50.0, row = None, debug = False,): [SOURCE]</p> <p>Description:Finds pixel positions of spectral lines in a provided image. The image does not need to range from any specific wavelength to another. The only requirement is that the lines are located vertically on the image plane. </p> <p>Parameters:image :: np.ndarrayImage with calibration lines.std_variation :: floatHow many standard deviations a peak must exceed the baseline to     be counted as a line.row :: intWhich row to pull a 1D profile from. If no argument is     provided, this function will default to the row at the center     of the cross-dispersion axis.debug :: boolAllows for a diagnostic plot to be shown.</p> <p>Returns:non_zero_indices :: np.ndarrayList of pixel locations for detected lines.rel_intensity :: np.ndarrayList of the relative intensity of the detected lines.</p>"},{"location":"docstring_documentation/#generate_effpix_map","title":"generate_effpix_map","text":"<p> specsuite.warping.generate_effpix_map ( xs, rows, models,): [SOURCE]</p> <p>Description:Generates a 2D map of the effective location of each pixel center across the detector. It uses models that describe the warping across the detector as a function of row and column positions. </p> <p>Parameters:xs :: np.ndarrayA 1D array holding all column pixel positions.rows :: np.ndarrayA 1D array holding all row pixel positions.models :: np.ndarrayAn array holding how light warping changes as a function of     column and row pixel positions. This function assumes that each     fitting coefficient changes according to a linear relationship.</p> <p>Returns:effpix_map :: np.ndarrayA 2D array detailing the effective location of each pixel center.</p>"},{"location":"docstring_documentation/#generate_warp_model","title":"generate_warp_model","text":"<p> specsuite.warping.generate_warp_model ( image, guess, tolerance = 16, line_order = 2, warp_order = 1, ref_idx = None, debug = False,): [SOURCE]</p> <p>Description:Models how straight vertical lines in a wavelength calibration image are being warped. Assumes a relatively low amount of straight-line warping and that these lines are continuous. This function allows for the type of warping to change along the horizontal axis of the detector. This is functionally achieved by binning down a given image and identifying the brightest pixels across the vertical axis of a detector in a small region around each line. </p> <p>Parameters:image :: np.ndarrayA 2D arclamp image with strong emission features.guess :: np.ndarrayThe pixel positions (along the dispersion axis) of strong     emissions features in the provided image. These can be slightly     wrong, as the 'tolerance' determines how broad of a region to     analyze around each of these guesses.tolerance :: intNumber of pixels around your guess to use for building a warp     model. The tolerance should at least be large enough that the     entire emission line falls within a given window, but it is     often better to provide a slightly larger tolerance if you     are uncertain in the accuracy of your guesses.line_order :: intOrder of x-warping (i.e. How does our vertial warping change     with x-position along the detector).warp_order :: intOrder of y-warping (i.e. What shape does a straight line     projected onto our detector take on).ref_idx :: intWhich row (along cross-dispersion axis) to use for measuring     the relative changes from row to row. This choice should be     near the brightest portion of the emission line.debug :: boolAllows for diagnostic plots to be displayed.</p> <p>Returns:warp_models :: listCollection of models describing how y-warping coefficients     change as a function of x.</p>"},{"location":"docstring_documentation/#_gaussian","title":"_gaussian","text":"<p> specsuite.utils._gaussian ( x, A, mu, sigma,): [SOURCE]</p> <p>Description:Generates a 1D Gaussian profile on the user-provided grid of x-points. If an error is encountered, then 'None' will be returned instead of a Numpy array. </p> <p>Parameters:x :: np.ndarrayA set of x-points over which to evaluate the Gaussian profile.     This can be a single value, but must still be contained in a     list (i.e., [1]).A :: floatThe amplitude of the Gaussian profile.mu :: floatThe mean of the Gaussian profile.sigma :: floatThe standard deviation of the Gaussian profile.</p> <p>Returns:profile :: np.ndarrayThe 1D Gaussian profile evaluated on the provided grid of x-points.</p>"},{"location":"docstring_documentation/#_moffat","title":"_moffat","text":"<p> specsuite.utils._moffat ( x, A, mu, gamma, offset = 0.0,): [SOURCE]</p> <p>Description:Generates a 1D Moffat profile on the user-provided grid of x-points. If an error is encountered, then 'None' will be returned instead of a Numpy array. Note: This is technically a 'modified Moffat profile' since the exponent has been set to 2.5. </p> <p>Parameters:x :: np.ndarrayA set of x-points over which to evaluate the Moffat profile.     This can be a single value, but must still be contained in a     list (i.e., [1]).A :: floatThe amplitude of the Moffat profile.mu :: floatThe mean of the Moffat profile.gamma :: floatA shape parameter for the Moffat profile.offset :: floatA constant offset applied to all points.</p> <p>Returns:profile :: np.ndarrayThe 1D Moffat profile evaluated on the provided grid of x-points.</p>"},{"location":"docstring_documentation/#flatfield_correction","title":"flatfield_correction","text":"<p> specsuite.utils.flatfield_correction ( image, flat, debug = False,): [SOURCE]</p> <p>Description:Applies a simple flatfield correction to one or more 2D images. This function assumes that each entry along the first axis is a 2D image with the same size as 'flat'. </p> <p>Parameters:image :: np.ndarrayImage(s) that should be divided by the normalized flatfield     image. This can be a single 2D image or an array of 2D images.flat :: np.ndarrayA single unnormalized flatfield image, ideally the median of     several flatfield exposures.debug :: boolAllows for diagnostic plotting.</p> <p>Returns:flatfielded_ims :: np.ndarrayThe resulting image(s) after being divided by the normalized     flatfield.</p>"},{"location":"docstring_documentation/#plot_image","title":"plot_image","text":"<p> specsuite.utils.plot_image ( image, xlim = None, ylim = None, xlabel = \"Dispersion Axis (pix)\", ylabel = \"Cross-Dispersion Axis (pix)\", cbar_label = \"Counts\", title = \"\", figsize = (10, 3), cmap = \"inferno\", savedir = \"None\", kwargs,): [SOURCE]</p> <p>Description:A simple wrapper for matplotlib.pyplot.imshow(). By default, this function uses a handful of style options to keep all visualizations consistent within our documentation. You should be able to overwrite these options and provide any of the standard additional KWARGS. </p> <p>Parameters:image :: np.ndarrayA single 2D array. If it is not a Numpy array, the function     will attempt to convert it into one.xlim :: tupleThe (xmin, xmax) to show. If none is provided, defaults to the     entire horizontal span of the image.ylim :: tupleThe (ymin, ymax) to show. If none is provided, defaults to the     entire vertical span of the image.xlabel :: strText to write along the x-axis (bottom) of the image.ylabel :: strText to write along the y-axis (left) of the image.cbar_label :: strA text label assigned to the colorbar.title :: strA title to plot at the top of the image.figsize :: tupleThe dimensions (horizontal, vertical) of the image.cmap :: strName of the matplotlib colormap to use.savedir :: strDirectory (+filename) to save the generated image at. If an argument     is provided, then 'plt.show()' will not run.</p> <p></p>"},{"location":"docstring_documentation/#rebin_image_columns","title":"rebin_image_columns","text":"<p> specsuite.utils.rebin_image_columns ( image, bin,): [SOURCE]</p> <p>Description:Rebins an image along a single axis. The bin size must be an integer multiple of the axis size being rebinned. </p> <p>Parameters:image :: np.ndarrayOriginal image to be rebinned.bin :: intSize each bin in pixels along the columns of the provided     image.</p> <p>Returns:rebinned_image :: np.ndarrayAn image where the columns have been rebinned into bin length     pixels.</p>"},{"location":"docstring_documentation/#generate_flux_conversion","title":"generate_flux_conversion","text":"<p> specsuite.throughput.generate_flux_conversion ( w_measured, w_model, f_measured, f_model, err, sigma_clip = 50.0, order = 7, max_iter = 50, debug = False,): [SOURCE]</p> <p>Description:Generates a numpy polynomial that predicts the physical flux [flam] / CCD count as a function of wavelength [Angstroms]. </p> <p>Parameters:w_measured :: np.ndarrayA 1D array of wavelengths for your CCD spectrum (in Angstroms).w_model :: np.ndarrayA 1D array of wavelengths for your known spectrum (in Angstroms).f_measured :: np.ndarrayA 1D array of flux for your CCD spectrum.f_model :: np.ndarrayA 1D array of flux for your known spectrum.err :: np.ndarrayA 1D array of errors for you CCD flux.sigma_clip :: floatThe max number of standard deviations a point is allowed to be     from the calibration model. Outlier points are removed from     future fits.order :: intPolynomial order of the flux conversion.max_iter :: intMaximum number of fits to perform before manually stopping.debug :: boolPlots the final fit against the user-provided data.</p> <p>Returns:p_flux_conversion :: np.poly1dAn n-th order polynomial that takes wavelength [Angstroms] as     an argument. The returned value converts CCD counts into     physical units [flam], meaning it is in units of flam/count.</p>"},{"location":"docstring_documentation/#load_stis_spectra","title":"load_STIS_spectra","text":"<p> specsuite.throughput.load_STIS_spectra ( name = \"None\", filetype = \"model\", wavelength_bounds = None, debug = False,): [SOURCE]</p> <p>Description:Attempts to download spectra data from the STIS website (see url below). It only looks for data contained in the first data table. </p> <p>Parameters:name :: strName of the star to load data for. This should match an entry     in the \"Star name\" column of Table 1.filetype :: strDetermines which type of model to load from the STIS database.     The only valid options are \"model\" or \"stis\".wavelength_bounds :: tupleThe (wmin, wmax) region of the STIS spectra to keep. Both     values must have astropy units compatible with wavelength.debug :: boolAllows diagnostic information to be output.</p> <p>Returns:wavs :: np.ndarrayRetrieved model wavelengths (Angstroms)flux :: np.ndarrayRetrieved model flux (flam)cont :: np.ndarrayRetrieved model flux (flam)</p>"},{"location":"docstring_documentation/#boxcar_extraction","title":"boxcar_extraction","text":"<p> specsuite.extraction.boxcar_extraction ( images, backgrounds, RN = 0.0, debug = False,): [SOURCE]</p> <p>Description:Performs a simple boxcar extraction on an image (or series of images). This assumes that both arrays of images of dimensions corresponding to...      (cross-dispersion, dispersion)  If that is not the case, please rotate your data arrays before feeding them into this function. </p> <p>Parameters:images :: np.ndarrayA 2D (or array of several 2D) science exposures that     have been background subtracted.backgrounds :: np.ndarrayA 2D (or array of several 2D) background exposures     that have been subtracted off of your science images.RN :: float | np.ndarrayThe read noise associated with your detector.debug :: boolAllows for optional plotting.</p> <p>Returns:flux_array :: np.ndarrayA 2D array containing the flux of each provided exposure.     Has a shape of (image index, pixel position).error_array :: np.ndarrayA 2D array containing the undertainty of each provided     exposure. Has a shape of (image index, pixel position).</p>"},{"location":"docstring_documentation/#generate_spatial_profile","title":"generate_spatial_profile","text":"<p> specsuite.extraction.generate_spatial_profile ( image, profile = \"moffat\", profile_order = 7, bin_size = 8, repeat = True, debug = False,): [SOURCE]</p> <p>Description:Generates a 'spatial profile' as outlined in Horne (1986). Spatial profiles predict the likelihood that a photon would land at a given cross-dispersion location for each wavelength. This function assumes that the dispersion axis is located along the x-axis. </p> <p>Parameters:image :: np.ndarrayThe image that a spatial profile is fit to.profile :: strName of the type of profile to fit for. Currently, the     only valid options are...         - moffat         - gaussianprofile_order :: intThe order of the polynomial used to fit to each constant     in the specified spatial profile (i.e., along the dispersion     axis, the mean evolve as what order of polynomial?)bin_size :: intSize of each bin used for 'binning down' the provided image     before fitting.repeat :: boolAllows the initial fit to each parameter to influence the     initial guesses in a second series of fits.debug :: boolAllows for optional debugging plots to be shown.</p> <p></p>"},{"location":"docstring_documentation/#horne_extraction","title":"horne_extraction","text":"<p> specsuite.extraction.horne_extraction ( images, backgrounds, profile = \"moffat\", profile_order = 3, RN = 0.0, bin_size = 16, max_iter = 5, repeat = True, debug = False, progress = False,): [SOURCE]</p> <p>Description:Performs a profile-weighted (Horne) extraction for a series of science exposures. </p> <p>Parameters:images :: np.ndarrayA single (or multiple) 2D exposures containing a point-source     trace to extract flux from.backgrounds :: np.ndarrayA single (or multiple) 2D exposures contianing the background     subtracted off of the science exposures. Used for calculating     the uncertainty of the reduction.profile :: strWhich type of 1D profile to use for generating a spatial     profile. Valid options are 'moffat' or 'gaussian'.profile_order :: intThe polynomial order that describes how the 1D profile (in the     fitted spatial profile) changes with pixel position along the     dispersion axis.RN :: float | np.ndarrayThe read noise of your exposure. If the provided argument is a     float, then every pixel will be assigned an equal read noise.     Otherwise, if provided a 2D array, then each exposure will be     assigned the corresponding value for that pixel.bin_size :: intThe number of pixels (dispersion axis) to lump into a single     bin when generating a spatial profile. Generally, a higher value     increases the probability that 'generate_spatial_profile()'     converges, but the precision of the extracted profile is lower.max_iter :: intThe number of iterations to repeat the Horne extraction algorithm     for. The cosmic ray masking has been removed, so the only benefit     from increasing 'max_iter' is the potential to get a better     constraint on the spatial profile.repeat :: boolWhether to repeat the spatial profile generation once an initial     pass has been made. When your data is particularly noisy, it is     helpful to keep this as 'True'.debug :: boolAllows for optional plotting.progress :: boolEnables a progress bar to be displayed.</p> <p>Returns:flux :: np.ndarrayAn array containing the extracted flux for each exposure.flux_err :: np.ndarrayAn array containing the extracted error for each exposure.</p>"},{"location":"docstring_documentation/#trace_fit","title":"trace_fit","text":"<p> specsuite.extraction.trace_fit ( image, bin = 16, trace_order = 2, debug = False,): [SOURCE]</p> <p>Description:Fits a trace to a signal across the horizontal axis of an image. This is done by rebinning a user-given image, fitting a gaussian to each rebinned column, and fitting an n-dimensional curve to these gaussian positions. </p> <p>Parameters:image :: np.ndarrayImage with a signal spanning the horizontal     axis of the detector.bin :: intNumber of pixels to group into a single bin.     Must be an integer multiple of the horizontal     pixel count.trace_order :: intOrder of the polynomial to be fit to our     trace fit data.debug :: boolAllows plot generation.</p> <p>Returns:xpoints :: np.ndarrayHorizontal pixel positions corresponding     to our detected trace fit. This has been     converted from the downsampled x-values     to the original image x-values.locs :: np.ndarrayVertical locations of the detected trace     positions.stds :: np.ndarrayStandard deviations associated with each     gaussian fit in the downsampled image.p_center :: np.poly1dPolynomial fit that traces our signal     out across the detector.</p>"},{"location":"docstring_documentation/#calculate_evidence","title":"calculate_evidence","text":"<p> specsuite.wavecal.calculate_evidence ( valid_keys, d_obs, sig_d, pad,): [SOURCE]</p> <p>Description:Calculates the Bayesian evidence (i.e., the probability of a given key being calculated integrated over all triplet pairs). </p> <p>Parameters:valid_keys :: np.ndarrayAll possibe keys (rounded distances) across all triplet     pairs.d_obs :: np.ndarrayThe calculated relative distances corresponding to each     triplet pair.sig_d :: np.ndarrayThe calculated error in each distance measurement provided     in 'd_obs'.pad :: floatThe size of half a bin in the geometric hashing routine.     This should be equivalent to 0.5 * 10^(-rounding).</p> <p>Returns:evidence :: np.ndarrayA 1D array containing the probability that each key is measured     across all possible triplet combinations.prob_mass :: np.ndarrayA 2D array containing the probability mass for all possible     triplet combinations.</p>"},{"location":"docstring_documentation/#cast_votes","title":"cast_votes","text":"<p> specsuite.wavecal.cast_votes ( n_model_points, n_data_points, prob_mass, evidence, valid_keys, hash_table, di_v, dj_v, dk_v,): [SOURCE]</p> <p>Description:Casts votes for each possible combination of points using a traditional geometric hashing scheme. If a pair of triplets produces a key found in the hash table, then the data points will be added as a possible match for each of model points listed for that entry in the hash table. However, the votes are weighted by the Bayesian probability of that match being correct. </p> <p>Parameters:n_model_points :: intThe total number of model points.n_data_points :: intThe total number of data points.prob_mass :: np.ndarrayA 2D array containing the probability mass for all possible     triplet combinations.evidence :: np.ndarrayA 1D array containing the probability that each key is measured     across all possible triplet combinations.valid_keys :: np.ndarrayA list of all possible keys in the hash table. This is stored     in a separate list to prevent unecessary duplicate lookups.hash_table :: dictA hash table where each key corresponds to the relative distance     calculated from three points.di_v :: np.ndarrayThe indices of all b1 values in the data triplet array.dj_v :: np.ndarrayThe indices of all b2 values in the data triplet array.dk_v :: np.ndarrayThe indices of all p values in the data triplet array.</p> <p>Returns:votes :: np.ndarrayThe probability-weighted votes indicating which pairs of model and     data points is most likely.</p>"},{"location":"docstring_documentation/#compute_triplet_values_from_indices","title":"compute_triplet_values_from_indices","text":"<p> specsuite.wavecal.compute_triplet_values_from_indices ( lines_round, triplet_idx,): [SOURCE]</p> <p>Description:Using a list of rounded line positions, this function calculates the relative distances of each possible triplet of points. It also returns some related, useful arrays that can be used for filtering. All calculations are vectorized to reduce runtime. </p> <p>Parameters:lines_round :: np.ndarrayA 1D array containing rounded line positions.triplet_idx :: np.ndarrayA 2D array (N x 3) containing the indices of each line that     make up the triplet. The length N should be equal to the total     number of valid triplets that can be formed from 'lines_round',     and each entry should be a value representing an index location     in the 'lines_round' array.</p> <p>Returns:b1 :: np.ndarrayA 1D array containing all of the first base points.b2 :: np.ndarrayA 1D array containing all of the second base points.p :: np.ndarrayA 1D array containing all of the reference points.d :: np.ndarrayA 1D array containing all of the scale-normalized distances     calculated using b1, b2, and p. If the calculated scale is     0.0 or the reference point is the same as one of the base     points, the corresponding entry is replaced with a NaN value.valid :: np.ndarrayA 1D array filled with boolean entries indicating whether the     calculated distance is a NaN or not.i :: np.ndarrayThe indices of all b1 values in the 'triplet_idx' array.j :: np.ndarrayThe indices of all b2 values in the 'triplet_idx' array.k :: np.ndarrayThe indices of all p values in the 'triplet_idx' array.</p>"},{"location":"docstring_documentation/#match_features","title":"match_features","text":"<p> specsuite.wavecal.match_features ( raw_data_lines, raw_model_lines, iterations = 10, rounding = 3, sigma = 2.0, order = 2, debug = False,): [SOURCE]</p> <p>Description:Attempts to find the most probable matches in lines bewteen two lists. This function assumes that the model lines have no (or negligible) error, and that all data lines have a constant error described by 'sigma'. </p> <p>Parameters:raw_data_lines :: np.ndarrayA 1D list of lines indicating the pixel positions     of line emissions.raw_model_lines :: np.ndarrayA 1D list of line indicating the known wavelengths     of some of the observed data lines. This list can     be shorter or longer than 'raw_data_lines,' but     this will impact the accuracy of the voting routine.iterations :: intThe number of iterations to repeat feature matching.     For well-behaved examples, the votes will converge     to a consistent answer after a handful of iterations.     However, excessive looping can lead to a gradual break     in the accuracy of the calculated matches.rounding :: intHow many decimal places to measure relative distances     to. For example, using 'rounding=3' will calculate     distances to three decimal places.sigma :: floatThe uncertainty (in the same units as 'raw_data_lines')     of data line locations.order :: intThe polynomial order to fit to the matched features. We     highly recommend keeping this at 'order=2'.debug :: boolAllows for diagnostic plots to be shown.</p> <p>Returns:votes :: np.ndarrayA 2D array showing the probability of a given data line     being paired with a given model line.</p>"},{"location":"docstring_documentation/#sigma_d_from_triplets","title":"sigma_d_from_triplets","text":"<p> specsuite.wavecal.sigma_d_from_triplets ( b1, b2, p, sigma_measured,): [SOURCE]</p> <p>Description:Calculates the uncertainty in relative distance measurements for a given triplet of points. </p> <p>Parameters:b1 :: np.ndarrayA 1D array containing all of the first base points.b2 :: np.ndarrayA 1D array containing all of the second base points.p :: np.ndarrayA 1D array containing all of the reference points.sigma_measured :: floatAn estimate of the uncertainty in line positions.     Assumes that the same error applies to all points.</p> <p>Returns:sigma_d :: np.ndarrayA 1D array representing the uncertainty in distance     measurements for each triplet.</p>"},{"location":"flux_calibration/","title":"Flux Calibration","text":"<p>For some types of observations, you may not need to know about the absolute physical flux of an object (e.g., normalized transit light curves). There are many situations where the precise physical flux is useful, though, necessitating a \"flux calibration.\" This calibration finds the wavelength-dependent conversion between detector counts and physical flux units.</p> <p>In this section, we show how <code>specsuite</code> can help automate this process for certain well-characterized flux calibrator stars.</p> <p>Note</p> <p>         In our \"Setup Code,\" we are loading in a CSV with a pre-generated wavelength solution.     </p> In\u00a0[1]: hide_code_block Copied! <pre>import specsuite as ss\nimport pandas as pd\nimport numpy as np\n\nDATA_REGION = (300, 600)\nCAL_PATH = \"../data/KOSMOS/calibrations\"\nDATA_PATH = \"../data/KOSMOS/target\"\nINSTRUMENT = \"kosmos\"\n\nbias = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"bias\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT,\n)\n\narclamp = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"neon\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT,\n) - bias\n\nflat = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"flat\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT\n) - bias\n\nfluxcal = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"fluxcal\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT,\n) - bias\n\n# Performs a simple flatfield correction\nfluxcal = ss.flatfield_correction(fluxcal, flat)\n\n# Generates a decent warp model for our sample KOSMOS data\nlocs, _ = ss.find_cal_lines(arclamp, std_variation=200)\nwarp_model = ss.generate_warp_model(arclamp, locs)\nbackground = ss.extract_background(fluxcal, warp_model, mask_region=(150, 200))[0]\nfluxcal -= background\n\n### Extracts flux from image\nflux_extracted, uncertainty_extracted = ss.boxcar_extraction(fluxcal, background, RN=6.0)\n\ndf = pd.read_csv(\"../data/KOSMOS/calibrations/APO_red_wavecal.csv\")\np_wavecal = np.poly1d(\n    np.polyfit(\n        x = df[\"pixel positions\"],\n        y = df[\"wavelengths (AA)\"],\n        deg = 2,\n    )\n)\n\npix = np.array(range(len(fluxcal[0])))\nmeasured_wavs = p_wavecal(pix)\n</pre> import specsuite as ss import pandas as pd import numpy as np  DATA_REGION = (300, 600) CAL_PATH = \"../data/KOSMOS/calibrations\" DATA_PATH = \"../data/KOSMOS/target\" INSTRUMENT = \"kosmos\"  bias = ss.average_matching_files(     path = CAL_PATH,     tag = \"bias\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT, )  arclamp = ss.average_matching_files(     path = CAL_PATH,     tag = \"neon\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT, ) - bias  flat = ss.average_matching_files(     path = CAL_PATH,     tag = \"flat\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT ) - bias  fluxcal = ss.average_matching_files(     path = CAL_PATH,     tag = \"fluxcal\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT, ) - bias  # Performs a simple flatfield correction fluxcal = ss.flatfield_correction(fluxcal, flat)  # Generates a decent warp model for our sample KOSMOS data locs, _ = ss.find_cal_lines(arclamp, std_variation=200) warp_model = ss.generate_warp_model(arclamp, locs) background = ss.extract_background(fluxcal, warp_model, mask_region=(150, 200))[0] fluxcal -= background  ### Extracts flux from image flux_extracted, uncertainty_extracted = ss.boxcar_extraction(fluxcal, background, RN=6.0)  df = pd.read_csv(\"../data/KOSMOS/calibrations/APO_red_wavecal.csv\") p_wavecal = np.poly1d(     np.polyfit(         x = df[\"pixel positions\"],         y = df[\"wavelengths (AA)\"],         deg = 2,     ) )  pix = np.array(range(len(fluxcal[0]))) measured_wavs = p_wavecal(pix) In\u00a0[2]: Copied! <pre>model_wavs, model_flux, continuum_flux = ss.load_STIS_spectra(\n    name = \"GD153\",\n    debug = True,\n)\n</pre> model_wavs, model_flux, continuum_flux = ss.load_STIS_spectra(     name = \"GD153\",     debug = True, ) <p>By default, this will load the SED for all available wavelengths. If you only need a specific range of wavelengths, you can specify it using an optional argument...</p> In\u00a0[3]: Copied! <pre>import astropy.units as u\n\nmodel_wavs, model_flux, _ = ss.load_STIS_spectra(\n    name = \"GD153\",\n    wavelength_bounds = [5000, 10000] * u.AA,\n    debug = True,\n)\n</pre> import astropy.units as u  model_wavs, model_flux, _ = ss.load_STIS_spectra(     name = \"GD153\",     wavelength_bounds = [5000, 10000] * u.AA,     debug = True, ) <p>Importantly, <code>load_STIS_spectra()</code> will load the model spectra by default. If you wanted a calibrated, measured SED for the star, you will need to indicate this using...</p> In\u00a0[4]: Copied! <pre>_ = ss.load_STIS_spectra(\n    name = \"GD153\",\n    filetype = \"stis\",\n    wavelength_bounds = [5000, 10000] * u.AA,\n    debug = True,\n)\n</pre> _ = ss.load_STIS_spectra(     name = \"GD153\",     filetype = \"stis\",     wavelength_bounds = [5000, 10000] * u.AA,     debug = True, ) <p>Note that loading the \"stis\" SED will return seven total arguments (rather than the default three). These are...</p> <ul> <li>Wavelength (Angstroms)</li> <li>Flux (flam)</li> <li>Statistical Error (flam)</li> <li>Systematic Error (flam)</li> <li>Full-Width Half Maximum (Angstroms)</li> <li>Data Quality</li> <li>Exposure Time (s)</li> </ul> <p>Only the first two of these are needed for our flux calibration routines.</p> In\u00a0[5]: Copied! <pre>p_flux = ss.generate_flux_conversion(\n    w_measured = measured_wavs, \n    w_model = model_wavs, \n    f_measured = flux_extracted, \n    f_model = model_flux,\n    err = uncertainty_extracted,\n    debug = True\n)\n</pre> p_flux = ss.generate_flux_conversion(     w_measured = measured_wavs,      w_model = model_wavs,      f_measured = flux_extracted,      f_model = model_flux,     err = uncertainty_extracted,     debug = True ) <p>The above plot shows your flux-calibrated data (black) against the provided model spectra (red). We can see that some features (such as the absorption feature around 7500 AA) still vary significantly from our model. These typically correspond to strong atmospheric absorptions that are not captured in the model spectra, and should not significantly impact the accuracy of the flux calibration.</p> <p><code>generate_flux_conversion()</code> returns a Numpy polynomial that takes wavelength (units of Angstroms) as an argument, and returns the conversion factor from detector counts to flam. We can take a look at this model by itself before applying it to our data...</p> In\u00a0[6]: Copied! <pre>import matplotlib.pyplot as plt\n\nconversion = p_flux(measured_wavs)\n\nplt.plot(measured_wavs, conversion)\nplt.xlabel(\"Wavelength [AA]\")\nplt.ylabel(\"Conversion Factor [flam / count]\")\nplt.yscale(\"log\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  conversion = p_flux(measured_wavs)  plt.plot(measured_wavs, conversion) plt.xlabel(\"Wavelength [AA]\") plt.ylabel(\"Conversion Factor [flam / count]\") plt.yscale(\"log\") plt.show() <p>Then, to apply this flux calibration to our data, we simply multiply an extracted SED by this conversion factor...</p> In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\n\nconversion = p_flux(measured_wavs) * flux_extracted[0]\n\nplt.plot(measured_wavs, conversion)\nplt.xlabel(\"Wavelength [AA]\")\nplt.ylabel(\"Flux [flam]\")\nplt.yscale(\"log\")\nplt.show()\n</pre> import matplotlib.pyplot as plt  conversion = p_flux(measured_wavs) * flux_extracted[0]  plt.plot(measured_wavs, conversion) plt.xlabel(\"Wavelength [AA]\") plt.ylabel(\"Flux [flam]\") plt.yscale(\"log\") plt.show() In\u00a0[8]: Copied! <pre>try:\n    _ = ss.load_STIS_spectra(\n        name = \"GD 153\",\n        filetype = \"stis\",\n        debug = True,\n    )\nexcept AssertionError as e:\n    print(e)\n</pre> try:     _ = ss.load_STIS_spectra(         name = \"GD 153\",         filetype = \"stis\",         debug = True,     ) except AssertionError as e:     print(e) <pre>Name 'GD 153' not found in table...\n</pre> <p>If you encounter this error, double-check that your provided <code>name</code> exactly matches an entry of Table 1 on the STIS website. If there is a match, but you are still encountering an error, then this may be an issue with our parser. Please tell us about this issue on the GitHub repository if you run into it!</p> In\u00a0[9]: Copied! <pre>_ = ss.load_STIS_spectra(\n    name = \"GD153\",\n    wavelength_bounds = [5000, 10000],\n    debug = True,\n)\n</pre> _ = ss.load_STIS_spectra(     name = \"GD153\",     wavelength_bounds = [5000, 10000],     debug = True, ) <pre>Wavelength bounds must be astropy.Quantities, not '&lt;class 'list'&gt;'\n</pre> <p>If you run into this error, please make sure to multiply your bounds by an astropy quantity with units of length. All of the following are valid examples...</p> In\u00a0[10]: Copied! <pre>_ = ss.load_STIS_spectra(\n    name = \"GD153\",\n    wavelength_bounds = [5000, 10000] * u.AA,\n)\n\n_ = ss.load_STIS_spectra(\n    name = \"GD153\",\n    wavelength_bounds = [5000 * u.AA, 10000 * u.AA],\n)\n\n_ = ss.load_STIS_spectra(\n    name = \"GD153\",\n    wavelength_bounds = [500, 1000] * u.nm,\n)\n</pre> _ = ss.load_STIS_spectra(     name = \"GD153\",     wavelength_bounds = [5000, 10000] * u.AA, )  _ = ss.load_STIS_spectra(     name = \"GD153\",     wavelength_bounds = [5000 * u.AA, 10000 * u.AA], )  _ = ss.load_STIS_spectra(     name = \"GD153\",     wavelength_bounds = [500, 1000] * u.nm, ) In\u00a0[11]: Copied! <pre>p_flux = ss.generate_flux_conversion(\n    w_measured = measured_wavs, \n    w_model = model_wavs, \n    f_measured = flux_extracted, \n    f_model = model_flux,\n    err = uncertainty_extracted,\n    order = 2,\n    debug = True\n)\n</pre> p_flux = ss.generate_flux_conversion(     w_measured = measured_wavs,      w_model = model_wavs,      f_measured = flux_extracted,      f_model = model_flux,     err = uncertainty_extracted,     order = 2,     debug = True ) <p>By default, the flux calibration will be a 7th order polynomial, but this may need some tweaking for your specific dataset. If you see that the red line (model) does a poor job describing your data, this should be your first check.</p> <p>Another potential issue comes from the iterative sigma-clipping portion of the function. If you find that certain non-stellar features (i.e., atmospheric absorption bands) are strongly impacting the polynomial fit, you may need to decrease the <code>sigma_clipping</code> threshold or increase the <code>max_iter</code> parameter.</p>"},{"location":"flux_calibration/#basic-usage","title":"Basic Usage\u00b6","text":""},{"location":"flux_calibration/#loading-model-spectra","title":"Loading Model Spectra\u00b6","text":"<p>The first step in a flux calibration involves retrieving the known SED of your calibration star (for our example, we will use GD153). If you do not have a model SED handy, it might be one of many publicly-available SED's listed on STScI's \"Calspec\" page (See Table 1). If your star is found on the linked page, you can quickly load the model SED into Python using the following code...</p>"},{"location":"flux_calibration/#generate-a-flux-conversion","title":"Generate a Flux Conversion\u00b6","text":"<p>Using the model SED, we can convert detector counts into physical flux units (in this example, flam). Assuming this conversion changes smoothly as a function of wavelength, we can model it using a low-order polynomial in <code>generate_flux_conversion()</code>! Let's look at an example for GD153...</p>"},{"location":"flux_calibration/#common-errors","title":"Common Errors\u00b6","text":""},{"location":"flux_calibration/#star-not-found-in-stis-table","title":"Star not found in STIS Table\u00b6","text":"<p>If you attempt to load a model SED from the STIS website, but the name is not found in Table 1, then <code>load_STIS_spectra()</code> will throw an error. This function is reading in the raw HTML of the linked page and attempting to parse all of the names in the \"Star name\" column. For this reason, the loading function is very sensitive to the exact spelling of your provided <code>name</code>. For example, if we insert an extra space in \"GD153\" we get...</p>"},{"location":"flux_calibration/#forgetting-to-specify-units","title":"Forgetting to specify units\u00b6","text":"<p>When specifying which portion of an STIS SED to load, it is easy to omit the units of your bounds. When you do so, it is impossible to infer which units you intended to provide, so we print a warning to the screen and return 'None'. For example...</p>"},{"location":"flux_calibration/#misshappen-flux-calibration","title":"Misshappen Flux Calibration\u00b6","text":"<p>In some situations, you may already know the broad shape of your flux calibration curve. If this shape does not match the <code>p_flux</code> generated by <code>generate_flux_conversion()</code>, one a several things may have gone wrong. The most common issue comes from a poor choice of <code>order</code>...</p>"},{"location":"flux_extraction/","title":"Flux Extraction","text":"<p>By this point, your data calibration should be nearly complete! The last step necessary for you to get a quick look at your extracted spectral energy distribution (SED) is called a \"flux extraction.\" Among modern reduction pipelines, a handful of flux extraction algorithms have become common (most common are the \"boxcar\" and \"optimal\" extraction algorithms).</p> <p>In this section, we discuss a few algorithms <code>specsuite</code> can use to extract flux from your calibrated science exposures.</p> In\u00a0[1]: hide_code_block Copied! <pre>import specsuite as ss\n\n# Defines where to look for data\nDATA_REGION = (700, 800)\nCAL_PATH = \"../data/KOSMOS/calibrations\"\nDATA_PATH = \"../data/KOSMOS/target\"\nINSTRUMENT = \"kosmos\"\n\n# Loads calibration exposures\nbias = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"bias\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT\n)\narclamp = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"neon\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT,\n) - bias\n\n# Loads science exposures\nscience = ss.collect_images_array(\n    DATA_PATH,\n    \"toi3884\",\n    crop_bds=DATA_REGION,\n    instrument = INSTRUMENT,\n) - bias\n\n# Extracts background emissions\nlocs, _ = ss.find_cal_lines(arclamp, std_variation = 200)\nwarp_model = ss.generate_warp_model(arclamp, locs)\nbackgrounds = ss.extract_background(science, warp_model, mask_region = (40, 80))\n\nscience -= backgrounds\n</pre> import specsuite as ss  # Defines where to look for data DATA_REGION = (700, 800) CAL_PATH = \"../data/KOSMOS/calibrations\" DATA_PATH = \"../data/KOSMOS/target\" INSTRUMENT = \"kosmos\"  # Loads calibration exposures bias = ss.average_matching_files(     path = CAL_PATH,     tag = \"bias\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT ) arclamp = ss.average_matching_files(     path = CAL_PATH,     tag = \"neon\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT, ) - bias  # Loads science exposures science = ss.collect_images_array(     DATA_PATH,     \"toi3884\",     crop_bds=DATA_REGION,     instrument = INSTRUMENT, ) - bias  # Extracts background emissions locs, _ = ss.find_cal_lines(arclamp, std_variation = 200) warp_model = ss.generate_warp_model(arclamp, locs) backgrounds = ss.extract_background(science, warp_model, mask_region = (40, 80))  science -= backgrounds In\u00a0[2]: Copied! <pre>boxcar_flux, boxcar_errs = ss.boxcar_extraction(\n    science,\n    backgrounds,\n    RN = 6.0,\n    debug = True,\n)\n</pre> boxcar_flux, boxcar_errs = ss.boxcar_extraction(     science,     backgrounds,     RN = 6.0,     debug = True, ) <p>Notice the prominant cosmic ray contamination and poor SNR for dispersion-axis pixels with a strong background emission. While neither can be entirely corrected for, the next extraction method uses a statistical argument to mitigate both of these non-ideal features.</p> In\u00a0[3]: Copied! <pre>P_moffat = ss.generate_spatial_profile(\n    image = science[0],\n    profile = \"moffat\",\n)\n\nss.plot_image(P_moffat, title = \"Moffat Spatial Profile\")\nss.plot_image(science[0], title = \"Science Exposure\")\n</pre> P_moffat = ss.generate_spatial_profile(     image = science[0],     profile = \"moffat\", )  ss.plot_image(P_moffat, title = \"Moffat Spatial Profile\") ss.plot_image(science[0], title = \"Science Exposure\") <p>If you want to use a Gaussian profile, that option is still available, but you may need to tweak some optional settings to get a high-quality fit. For example, for our KOSMOS data, we need to slightly increase the bin size and disable the \"repeat\" argument (this runs a second fitting routine to try and improve the quality of the 1D profile fits). Doing so gives us...</p> In\u00a0[4]: Copied! <pre>P_gaussian = ss.generate_spatial_profile(\n    image = science[0],\n    profile = \"gaussian\",\n    bin_size = 16,\n    repeat = False,\n)\n\nss.plot_image(P_gaussian, title = \"Gaussian Spatial Profile\")\nss.plot_image(science[0], title = \"Science Exposure\")\n</pre> P_gaussian = ss.generate_spatial_profile(     image = science[0],     profile = \"gaussian\",     bin_size = 16,     repeat = False, )  ss.plot_image(P_gaussian, title = \"Gaussian Spatial Profile\") ss.plot_image(science[0], title = \"Science Exposure\") <p>Depending on your specific data / analysis, either of these could be a suitable choice! So long as the spatial profile appears to capture the shape of your signal reasonably well, the profile-weighted extraction should work as expected.</p> <p>Tip</p> <p>         For ground-based data, a Gaussian spatial profile is usually inaccurate. This is due to atmospheric effects blurring and is most significant for the wings of the profile. Since some modern reductions have found success with using a Moffat profile instead, we generally default to that choice as well. If you are processing data from a ground-based observatory and notice odd residuals toward the wings of your trace, double-check that you are using a Moffat profile in your reduction.     </p> <p>Warning</p> <p>         The Moffat profile we use is not completely accurate. For reference, the Moffat profile adopted by astropy is...         \\begin{equation}         f(x) = A \\left[ \\frac{(x - x_0)^2}{\\gamma^2} \\right]^{-\\alpha}.         \\end{equation}         If gamma and alpha were both left as free parameters, we found that fits struggled to converge to a single, consistent answer. We address this by setting alpha to a constant (2.5). This choice is arbitrary, but we have found that it consistently results in a higher quality of fit.     </p> In\u00a0[5]: Copied! <pre>optimal_flux, optimal_err = ss.horne_extraction(\n    images = science[0],\n    backgrounds = backgrounds[0],\n    profile = \"moffat\",\n    RN = 6.0,\n    debug = True,\n)\n</pre> optimal_flux, optimal_err = ss.horne_extraction(     images = science[0],     backgrounds = backgrounds[0],     profile = \"moffat\",     RN = 6.0,     debug = True, ) In\u00a0[6]: Copied! <pre>P_gaussian = ss.generate_spatial_profile(\n    image = science[0],\n    profile = \"gaussian\",\n)\n\nss.plot_image(P_gaussian, title = \"Gaussian Spatial Profile\")\nss.plot_image(science[0], title = \"Science Exposure\")\n</pre> P_gaussian = ss.generate_spatial_profile(     image = science[0],     profile = \"gaussian\", )  ss.plot_image(P_gaussian, title = \"Gaussian Spatial Profile\") ss.plot_image(science[0], title = \"Science Exposure\") <p>We can see that something weird is happening at the leftmost edge of the spatial profile. This is a situation where the optional debugging plots could be useful...</p> In\u00a0[7]: Copied! <pre>P_gaussian = ss.generate_spatial_profile(\n    image = science[0],\n    profile = \"gaussian\",\n    debug = True,\n)\n</pre> P_gaussian = ss.generate_spatial_profile(     image = science[0],     profile = \"gaussian\",     debug = True, ) <p>Here we see the polynomial fits to each Gaussian parameters (amplitude, mean, sigma) for each location along the dispersion axis. This clearly indicates that some erroneous fits at the left end of the detector are strongly biasing our spatial profile. To fix this, we can try messing with some of <code>generate_spatial_profile()</code>'s the optional parameters. In this particular case, simply using <code>repeat=False</code> is enough to cause a significant jump in quality!</p> In\u00a0[8]: Copied! <pre>P_gaussian = ss.generate_spatial_profile(\n    image = science[0],\n    profile = \"gaussian\",\n    repeat = False,\n    debug = True,\n)\n\nss.plot_image(P_gaussian)\n</pre> P_gaussian = ss.generate_spatial_profile(     image = science[0],     profile = \"gaussian\",     repeat = False,     debug = True, )  ss.plot_image(P_gaussian) <p>If you are still running into issues, try changing the bin size and order of the polynomial being fit to each profile parameter.</p> In\u00a0[9]: Copied! <pre>flux, err = ss.boxcar_extraction(\n    images = science,\n    backgrounds = backgrounds,\n    RN = 1.0,\n    debug = True,\n)\n\nflux, err = ss.boxcar_extraction(\n    images = science,\n    backgrounds = backgrounds,\n    RN = 10,\n    debug = True,\n)\n\nflux, err = ss.boxcar_extraction(\n    images = science,\n    backgrounds = backgrounds,\n    RN = 100,\n    debug = True,\n)\n</pre> flux, err = ss.boxcar_extraction(     images = science,     backgrounds = backgrounds,     RN = 1.0,     debug = True, )  flux, err = ss.boxcar_extraction(     images = science,     backgrounds = backgrounds,     RN = 10,     debug = True, )  flux, err = ss.boxcar_extraction(     images = science,     backgrounds = backgrounds,     RN = 100,     debug = True, ) <p>If you feel your errors are too small or large, verify that your value for read noise is correct. Additionally, double-check that your instrument's gain has been properly accounted for!</p> In\u00a0[10]: Copied! <pre>flux, err = ss.horne_extraction(\n    science + backgrounds,\n    backgrounds,\n    debug = True,\n)\n</pre> flux, err = ss.horne_extraction(     science + backgrounds,     backgrounds,     debug = True, ) <p>These large spikes / blocky features are caused by background emissions that are contaminating the extraction. This is fixed by simply subtracting off the background exposures beforehand.</p>"},{"location":"flux_extraction/#basic-usage","title":"Basic Usage\u00b6","text":"<p>As of now, there are only two available methods for extracting flux from your science exposure.</p>"},{"location":"flux_extraction/#boxcar-extraction","title":"Boxcar Extraction\u00b6","text":"<p>Let D(x, y) represent a single background-contaminated exposure where the values at every point has units of photo-electrons. Similarly, let S(x, y) represent the strength of the background across the image (also in units of photo-electrons). The simplest possible estimate of our signal is known as a boxcar extraction where...</p> <p>\\begin{equation*}     f(x) = \\sum_{i} \\left[ \\textbf{D}(x, y_i) - \\textbf{S}(x, y_i) \\right] \\end{equation*}</p> <p>is the extracted flux. The uncertainty of this signal is given by...</p> <p>\\begin{equation*}     \\sigma(x) = \\sqrt{RN^2 + \\sum_{i} \\textbf{D}(x, y_i)} \\end{equation*}</p> <p>where RN is the \"read noise\" of the detector. This is a quick and simple extraction method, but has several noteable drawbacks. Firstly, if cosmic ray contamination is present in your data, then the extracted flux will have erroneous and sudden jumps in brigthness for dispersion-axis pixels with any level of contamination. Secondly, the uncertainty of your extracted spectra is artificially inflated by dimly-illuminated regions of your detector.</p> <p>Despite these issues, it can be a useful for first looks! Here is a simple example call for our KOSMOS data...</p>"},{"location":"flux_extraction/#what-is-hornes-optimal-extraction","title":"What is Horne's (Optimal) Extraction?\u00b6","text":"<p>Horne's extraction routine uses an iterative process to revise its estimate of the variance (V(x, y)) until an optimal SNR is reached. Please refer to Horne (1986) for a complete discussion of this process. One important component of the optimal extraction routine is the spatial profile P. This profile describes how likely a photon from a point-source target is to land at any pixel along the cross-dispersion axis.</p> <p>We assume that each trace profile is well described by a Moffat or Gaussian profile, and that the parameters of the profile changes smoothly along the dispersion axis. If true, we are able to fit a 1D profile to each column in a science exposure, then fit a low-order polynomial to predict how each profile parameter changes along the dispersion axis. In practice, this can process can fail for faint columns and columns with significant cosmic ray contamination. We address both of these by fitting 1D profiles to a binned version of the science exposure.</p>"},{"location":"flux_extraction/#finding-a-spatial-profile","title":"Finding a Spatial Profile\u00b6","text":"<p>For ground-based data, we have found a Moffat profile to be a better description of our data than a Gaussian profile. Here is the extracted spatial profile for a single exposure using a Moffat profile...</p>"},{"location":"flux_extraction/#profile-weighted-extraction","title":"Profile-Weighted Extraction\u00b6","text":"<p>In Horne's optimal extraction algorithm, the flux is given by... $$ \\begin{equation} f(x) = \\frac{\\sum_y \\textbf{M} \\textbf{P} (\\textbf{D} - \\textbf{S}) / \\textbf{V}}{\\sum_y \\textbf{M} \\textbf{P}^2 / \\textbf{V}} \\end{equation} $$ with a variance... $$ \\begin{equation} \\text{var}[f](x) = \\frac{\\sum_y \\textbf{M} \\textbf{P}}{\\sum_y \\textbf{M} \\textbf{P}^2 / \\textbf{V}}. \\end{equation} $$ We decided to remove the outlier masking during our extraction. This means that cosmic ray contamination (and any other source of outliers) will no longer be masked out in the way outlined in Horne's original paper. Even without the cosmic ray masking, the quality of the extracted SED is significantly highter...</p>"},{"location":"flux_extraction/#common-errors","title":"Common Errors\u00b6","text":""},{"location":"flux_extraction/#bad-spatial-profile","title":"Bad Spatial Profile\u00b6","text":"<p>If you are using the <code>horne_extraction()</code>, it is important to make sure that the spatial profiles <code>specsuite</code> is generating are accurate. For example, the following spatial profile would result in an extremely poor flux extraction...</p>"},{"location":"flux_extraction/#not-providing-read-noise","title":"Not Providing Read Noise\u00b6","text":"<p>Both <code>boxcar_extraction()</code> and <code>horne_extraction()</code> assume that your image has a read noise of 0. This choice was made to prevent one detector's read noise from accidentally being used for other detectors. While we could make \"RN\" a required argument for both functions, you may wish to get a quick look at your data before you know the read noise of your detector.</p> <p>To demonstrate this, here are three example extractions using various values for the read noise...</p>"},{"location":"flux_extraction/#forgetting-to-subtract-background","title":"Forgetting to Subtract Background\u00b6","text":"<p>We assume that your <code>images</code> have already been background-subtracted. If you did not subtract off your backgrounds, you may end up with an extracted flux that looks like this...</p>"},{"location":"gmos_example/","title":"GMOS-N","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport specsuite as ss\nimport numpy as np\n</pre> import matplotlib.pyplot as plt import specsuite as ss import numpy as np In\u00a0[2]: Copied! <pre>CAL_PATH = \"../data/GMOS/calibrations\"\nDATA_PATH = \"../data/GMOS/target\"\nDATA_REGION = (1750, 1950)\n\n# Loads standard calibration images\nfull_bias = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"bias\",\n    instrument = \"GMOS\",\n    crop_bds = DATA_REGION,\n)\nfull_flat = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"flat\", \n    instrument=\"GMOS\",\n    crop_bds = DATA_REGION,\n)\nfull_arc = ss.average_matching_files(\n    path = \"../data/GMOS/calibrations\",\n    tag = \"CuAr\",\n    instrument = \"GMOS\",\n    crop_bds = DATA_REGION,\n)\n\n# Collects individual science exposures\nfull_science = ss.collect_images_array(\n    path = DATA_PATH,\n    tag = \"toi3884\",\n    instrument = \"GMOS\",\n    crop_bds = DATA_REGION,\n)\n\n# Loads the RN image for a single GMOS-N exposure\n_, full_RN = ss.loading._GMOS_loader(\n    path = \"../data/GMOS/target\",\n    file = \"toi3884.0017.fits\",\n    return_RN = True,\n)\nfull_RN = full_RN[DATA_REGION[0]:DATA_REGION[1]]\n</pre> CAL_PATH = \"../data/GMOS/calibrations\" DATA_PATH = \"../data/GMOS/target\" DATA_REGION = (1750, 1950)  # Loads standard calibration images full_bias = ss.average_matching_files(     path = CAL_PATH,     tag = \"bias\",     instrument = \"GMOS\",     crop_bds = DATA_REGION, ) full_flat = ss.average_matching_files(     path = CAL_PATH,     tag = \"flat\",      instrument=\"GMOS\",     crop_bds = DATA_REGION, ) full_arc = ss.average_matching_files(     path = \"../data/GMOS/calibrations\",     tag = \"CuAr\",     instrument = \"GMOS\",     crop_bds = DATA_REGION, )  # Collects individual science exposures full_science = ss.collect_images_array(     path = DATA_PATH,     tag = \"toi3884\",     instrument = \"GMOS\",     crop_bds = DATA_REGION, )  # Loads the RN image for a single GMOS-N exposure _, full_RN = ss.loading._GMOS_loader(     path = \"../data/GMOS/target\",     file = \"toi3884.0017.fits\",     return_RN = True, ) full_RN = full_RN[DATA_REGION[0]:DATA_REGION[1]] <p>Let's also go ahead and plot these exposures...</p> In\u00a0[3]: Copied! <pre># Plots all the loaded exposures\nss.plot_image(\n    full_RN,\n    title=\"GMOS-N RN Exposure\",\n)\nss.plot_image(\n    full_bias,\n    title = \"GMOS-N Bias Exposure\",\n)\nss.plot_image(\n    full_flat,\n    norm = 'log',\n    title = \"GMOS-N Flatfield Exposure\",\n)\nss.plot_image(\n    full_arc,\n    norm = 'log',\n    title = \"GMOS-N Arclamp (CuAr) Exposure\",\n)\nss.plot_image(\n    full_science[0],\n    norm = 'log',\n    title = \"GMOS-N Science Exposure\",\n)\n</pre> # Plots all the loaded exposures ss.plot_image(     full_RN,     title=\"GMOS-N RN Exposure\", ) ss.plot_image(     full_bias,     title = \"GMOS-N Bias Exposure\", ) ss.plot_image(     full_flat,     norm = 'log',     title = \"GMOS-N Flatfield Exposure\", ) ss.plot_image(     full_arc,     norm = 'log',     title = \"GMOS-N Arclamp (CuAr) Exposure\", ) ss.plot_image(     full_science[0],     norm = 'log',     title = \"GMOS-N Science Exposure\", ) <p>We can see that these \"chip gaps\" appear as entirely white columns in our data. For data that looks like this, it is often simplest to split up each chip and process them individually. For simplicity, we will only process data from the rightmost chip.</p> In\u00a0[4]: Copied! <pre>chip_idx = 1\n\n# Only keeps data from the rightmost chip\nRN = ss.split_chips(full_RN)[0][chip_idx]\nbias = ss.split_chips(full_bias)[0][chip_idx]\nraw_arc = ss.split_chips(full_arc)[0][chip_idx]\nraw_flat = ss.split_chips(full_flat)[0][chip_idx]\nraw_science = ss.split_chips(full_science)[:, chip_idx]\n\nss.plot_image(RN, title=\"GMOS-N Bias Exposure\")\nss.plot_image(bias, norm='log', title=\"GMOS-N Bias Exposure\")\nss.plot_image(raw_arc, norm='log', title=\"GMOS-N Arclamp (CuAr) Exposure\")\nss.plot_image(raw_flat, norm='log', title=\"GMOS-N Flatfield Exposure\")\nss.plot_image(raw_science[0], norm='log', title=\"GMOS-N Science Exposure\")\n</pre> chip_idx = 1  # Only keeps data from the rightmost chip RN = ss.split_chips(full_RN)[0][chip_idx] bias = ss.split_chips(full_bias)[0][chip_idx] raw_arc = ss.split_chips(full_arc)[0][chip_idx] raw_flat = ss.split_chips(full_flat)[0][chip_idx] raw_science = ss.split_chips(full_science)[:, chip_idx]  ss.plot_image(RN, title=\"GMOS-N Bias Exposure\") ss.plot_image(bias, norm='log', title=\"GMOS-N Bias Exposure\") ss.plot_image(raw_arc, norm='log', title=\"GMOS-N Arclamp (CuAr) Exposure\") ss.plot_image(raw_flat, norm='log', title=\"GMOS-N Flatfield Exposure\") ss.plot_image(raw_science[0], norm='log', title=\"GMOS-N Science Exposure\") In\u00a0[5]: Copied! <pre># Subtracts off bias from all exposures\narc = raw_arc - bias\nflat = raw_flat - bias\nscience = raw_science - bias\n\n# Performs a flatfield correction for all science exposures\nscience = ss.flatfield_correction(\n    image = science,\n    flat = flat,\n    debug = True,\n)\n</pre> # Subtracts off bias from all exposures arc = raw_arc - bias flat = raw_flat - bias science = raw_science - bias  # Performs a flatfield correction for all science exposures science = ss.flatfield_correction(     image = science,     flat = flat,     debug = True, ) <p>Unless there is a notable oddity in your flatfield or bias (see the KOSMOS example), there should not be anything to double-check during this step! If you run into odd results further along your pipeline, checking that these corrections work as you expect them to is a good first step.</p> <p>Note</p> <p>         For many telescopes, you may also take a series of \"dark\" exposures to see how much light your instrument gives off when not illuminated. While this is typically good practice, dark current contamination is too low to significantly impact our reduction. The technical documentation estimates that dark current only produces a single DN of contamination over roughly an hour-long exposure.     </p> In\u00a0[6]: Copied! <pre>locs, _ = ss.find_cal_lines(\n    image = arc,\n    std_variation = 6,\n    debug = True,\n)\n\nwarp_model = ss.generate_warp_model(\n    image = arc,\n    guess = locs,\n    debug = True,\n)\n</pre> locs, _ = ss.find_cal_lines(     image = arc,     std_variation = 6,     debug = True, )  warp_model = ss.generate_warp_model(     image = arc,     guess = locs,     debug = True, ) <p>We can see that the white grid seems to do a good job tracing out the shape of line emissions in our arclamp exposure, so our warp model is reasonably accurate! Let's go ahead and extract the background from our science exposures...</p> In\u00a0[7]: Copied! <pre>backgrounds = ss.extract_background(\n    images = science,\n    warp_model = warp_model,\n    mask_region = (75, 150),\n    debug = True,\n)\n</pre> backgrounds = ss.extract_background(     images = science,     warp_model = warp_model,     mask_region = (75, 150),     debug = True, ) <p>Since the region of the image that our target does not illuminate looks like random static, we can be reasonably certain that our background extraction is accurate!</p> In\u00a0[8]: Copied! <pre>boxcar_flux, boxcar_error = ss.boxcar_extraction(\n    images = (science - backgrounds),\n    backgrounds = backgrounds,\n    RN = RN,\n    debug = True,\n)\n</pre> boxcar_flux, boxcar_error = ss.boxcar_extraction(     images = (science - backgrounds),     backgrounds = backgrounds,     RN = RN,     debug = True, ) <p>You may notice that the \"first exposure\" (black) is consistently higher than the \"median exposure.\" This is an odd feature in this particular dataset, but should not significantly impact the following visualizations. If we perform the profile-weighted exposure, we will see the same trend...</p> In\u00a0[9]: Copied! <pre>horne_flux, horne_error = ss.horne_extraction(\n    images = science - backgrounds,\n    backgrounds = backgrounds,\n    RN = RN,\n    debug = True,\n)\n</pre> horne_flux, horne_error = ss.horne_extraction(     images = science - backgrounds,     backgrounds = backgrounds,     RN = RN,     debug = True, ) <p>Compared against the boxcar extraction, the data appears to be slightly less noisy! To help visualize this difference, let's take a look at the extracted SNR for each of these extractions...</p> In\u00a0[10]: Copied! <pre># Plots Boxcar Extraction\nss.plot_image(\n    boxcar_flux/boxcar_error,\n    title = \"Boxcar Extraction SNR\",\n    xlabel = \"Pixel Position (Dispersion Axis)\",\n    ylabel = \"Exposure Index\",\n    vmin = 80,\n    vmax = 300,\n)\n\n# Plots Horne Extraction\nss.plot_image(\n    horne_flux/horne_error,\n    title = \"Horne Extraction SNR\",\n    xlabel=\"Pixel Position (Dispersion Axis)\",\n    ylabel=\"Exposure Index\",\n    vmin = 80,\n    vmax = 300,\n)\n</pre> # Plots Boxcar Extraction ss.plot_image(     boxcar_flux/boxcar_error,     title = \"Boxcar Extraction SNR\",     xlabel = \"Pixel Position (Dispersion Axis)\",     ylabel = \"Exposure Index\",     vmin = 80,     vmax = 300, )  # Plots Horne Extraction ss.plot_image(     horne_flux/horne_error,     title = \"Horne Extraction SNR\",     xlabel=\"Pixel Position (Dispersion Axis)\",     ylabel=\"Exposure Index\",     vmin = 80,     vmax = 300, ) <p>While the profile-weighted extraction produces a higher SNR across all wavelengths, the benefit is most noticeable for blue (left) wavelengths! We will explore this trend more in the next section.</p> In\u00a0[11]: Copied! <pre>for chip_idx in range(3):\n\n    # Pulls the data for a single chip\n    RN = ss.split_chips(full_RN)[0][chip_idx]\n    arc = ss.split_chips(full_arc)[0][chip_idx]\n    bias = ss.split_chips(full_bias)[0][chip_idx]\n    flat = ss.split_chips(full_flat)[0][chip_idx]\n    science = ss.split_chips(full_science)[:, chip_idx]\n\n    # Subtracts off bias from all exposures\n    arc -= bias\n    flat -= bias\n    science -= bias\n\n    # Performs a flatfield correction for all science exposures\n    science = ss.flatfield_correction(\n        image = science,\n        flat = flat,\n    )\n\n    # Constructs a warp model using the arclamp exposure\n    locs, _ = ss.find_cal_lines(\n        image = arc,\n        std_variation = 6,\n    )\n    warp_model = ss.generate_warp_model(\n        image = arc,\n        guess = locs,\n    )\n\n    # Extracts the background emissions\n    backgrounds = ss.extract_background(\n        images = science,\n        warp_model = warp_model,\n        mask_region = (75, 150),\n    )\n\n    # Performs and Horne extraction to all images\n    flux, error = ss.boxcar_extraction(\n        images = science - backgrounds,\n        backgrounds = backgrounds,\n        RN = RN,\n    )\n    boxcar_flux = np.median(flux, axis=0)\n    boxcar_error = np.median(error, axis=0)\n\n    # Performs and Horne extraction to all images\n    flux, error = ss.horne_extraction(\n        images = science - backgrounds,\n        backgrounds = backgrounds,\n        RN = RN,\n    )\n    horne_flux = np.median(flux, axis=0)\n    horne_error = np.median(error, axis=0)\n\n    # Determines the pixel locations for each chip\n    xs = np.array(range(len(horne_flux)))\n    if chip_idx == 1:\n        xs += 61 + len(xs)\n    if chip_idx ==2:\n        xs += 2*61 + 2*len(xs)\n\n    plt.plot(\n        xs,\n        np.abs((horne_flux/horne_error)/(boxcar_flux/boxcar_error)),\n        color='black',\n    )\n\nplt.ylim(1, 6)\nplt.title(\"How Much Better is a Horne Extraction than a Boxcar?\")\nplt.xlabel(\"Pixel Position (dispersion axis)\")\nplt.ylabel(\"Horne SNR / Boxcar SNR\")\nplt.show()\n</pre> for chip_idx in range(3):      # Pulls the data for a single chip     RN = ss.split_chips(full_RN)[0][chip_idx]     arc = ss.split_chips(full_arc)[0][chip_idx]     bias = ss.split_chips(full_bias)[0][chip_idx]     flat = ss.split_chips(full_flat)[0][chip_idx]     science = ss.split_chips(full_science)[:, chip_idx]      # Subtracts off bias from all exposures     arc -= bias     flat -= bias     science -= bias      # Performs a flatfield correction for all science exposures     science = ss.flatfield_correction(         image = science,         flat = flat,     )      # Constructs a warp model using the arclamp exposure     locs, _ = ss.find_cal_lines(         image = arc,         std_variation = 6,     )     warp_model = ss.generate_warp_model(         image = arc,         guess = locs,     )      # Extracts the background emissions     backgrounds = ss.extract_background(         images = science,         warp_model = warp_model,         mask_region = (75, 150),     )      # Performs and Horne extraction to all images     flux, error = ss.boxcar_extraction(         images = science - backgrounds,         backgrounds = backgrounds,         RN = RN,     )     boxcar_flux = np.median(flux, axis=0)     boxcar_error = np.median(error, axis=0)      # Performs and Horne extraction to all images     flux, error = ss.horne_extraction(         images = science - backgrounds,         backgrounds = backgrounds,         RN = RN,     )     horne_flux = np.median(flux, axis=0)     horne_error = np.median(error, axis=0)      # Determines the pixel locations for each chip     xs = np.array(range(len(horne_flux)))     if chip_idx == 1:         xs += 61 + len(xs)     if chip_idx ==2:         xs += 2*61 + 2*len(xs)      plt.plot(         xs,         np.abs((horne_flux/horne_error)/(boxcar_flux/boxcar_error)),         color='black',     )  plt.ylim(1, 6) plt.title(\"How Much Better is a Horne Extraction than a Boxcar?\") plt.xlabel(\"Pixel Position (dispersion axis)\") plt.ylabel(\"Horne SNR / Boxcar SNR\") plt.show() <p>We can see that the SNR the profile-weighted extraction produces a SNR that is anywhere from ~1.5x greater (red to NIR) to ~5x greater (blue visible) than that of the boxcar extraction. In general, the boxcar extraction is most useful for getting a quick look at your data, and the Horne extraction should be used for your final, analysis-ready reduction.</p> In\u00a0[12]: Copied! <pre># Fill with your own data\npixel_list = [...]\nwavelength_list = [...]\n\n# Initialize widget\nwavecal_widget = ss.WavecalWidget(\n    upper_lines = pixel_list,\n    lower_lines = wavelength_list,\n)\n\n#wavecal_widget.activate_widget()   &lt;--- Uncomment this to use widget\n</pre> # Fill with your own data pixel_list = [...] wavelength_list = [...]  # Initialize widget wavecal_widget = ss.WavecalWidget(     upper_lines = pixel_list,     lower_lines = wavelength_list, )  #wavecal_widget.activate_widget()   &lt;--- Uncomment this to use widget <p></p>"},{"location":"gmos_example/#what-is-gmos-n","title":"What is GMOS-N?\u00b6","text":"<p>Attached to the 8.0-meter telescope at Gemini North Telescope at Mauna Kea, Hawaii is the Gemini Multi-Object Spectrograph (GMOS-N). This is a long-slit / multi-slit multi-object spectograph capable of observing from blue-visible into near-infrared wavelengths of light with a resolving power ranging from 210-8800 (depending on instrument configuration). Since this is a ground-based telescope, it is subject to the time-dependent contamination of Earth's atmosphere.</p> <p>In the following reduction, we process five exposures of TOI-3884 using GMOS-N. We will not perform a wavelength calibration (since this relies on interactive code), and we will not perform a flux calibration (in an attempt to keep as little data as possible on the GitHub repository).</p>"},{"location":"gmos_example/#reduction","title":"Reduction\u00b6","text":""},{"location":"gmos_example/#loading-data","title":"Loading Data\u00b6","text":"<p>Although you are able to load data using the same functions shown previously in the documentation, there are some extra complications that GMOS-N presents during the loading process. Firstly, the FITS files produced by GMOS-N have twelve separate headers, each containing a portion of the complete readout. Secondly, the detector is comprised of three chips that are separated by a gap (~61 pixels in length). We need to represent these gaps using NaN values to prevent logical errors during the wavelength calibration, but we also need to mask those out before further analysis (since many of the processing functions were not made to handle NaN columns). Let's start by just loading in the relevant data...</p>"},{"location":"gmos_example/#standard-calibrations","title":"Standard Calibrations\u00b6","text":"<p>With loading / splitting chips out of the way, we can perform some basic calibrations on our data! For this data, that just involves subtracting off the median bias exposure and performing a flatfield correction...</p>"},{"location":"gmos_example/#background-extraction","title":"Background Extraction\u00b6","text":"<p>Next, we will generate a \"warp model\" to use in a background extraction. Since GMOS-N is a fairly sensitive instrument, the threshold needed to identify line emissions in our arclamp exposure is much lower. A value of 6 works pretty well!</p>"},{"location":"gmos_example/#flux-extraction","title":"Flux Extraction\u00b6","text":"<p>There are currently two method available for extracting flux from your science exposures. These correspond with the standard \"boxcar\" and \"horne (optimal)\" extractions used in many other reduction pipelines. Since the boxcar extraction is less likely to hit a snag and fail, let's start there...</p>"},{"location":"gmos_example/#processing-all-three-chips","title":"Processing All Three Chips\u00b6","text":"<p>As of now, the easiest way to process multiple GMOS-N chips is to treat them each as individual detectors, then combine each reduced SED at the end. Below, we will reduce data for all three chips in a large loop...</p>"},{"location":"gmos_example/#what-comes-next","title":"What Comes Next?\u00b6","text":"<p>Once you have an extracted SED, your next steps will depend greatly on the purpose of your observation. If you care about the flux-calibrated SED of your target, then you will need to use some of the functions outlined in <code>Additional Tools &gt; Flux Calibration</code>. If you are only interested in the relative changes in brightness for your target, then you may need to repeat the above reduction for another in-slit comparison star.</p> <p>Regardless, you will likely need to perform a wavelength calibration to convert pixel position into units of wavelength. Consider using our interactive widget for this!</p>"},{"location":"kosmos_example/","title":"KOSMOS","text":"In\u00a0[1]: Copied! <pre>import matplotlib.pyplot as plt\nimport specsuite as ss\nimport numpy as np\n</pre> import matplotlib.pyplot as plt import specsuite as ss import numpy as np In\u00a0[2]: Copied! <pre>CAL_PATH = \"../data/KOSMOS/calibrations\"\nDATA_PATH = \"../data/KOSMOS/target\"\nDATA_REGION = (175, 300)\n\n# Loads standard calibration images\nbias = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"bias\",\n    instrument = \"KOSMOS\",\n    crop_bds = DATA_REGION,\n)\nraw_flat = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"flat\", \n    instrument=\"KOSMOS\",\n    crop_bds = DATA_REGION,\n)\nraw_arc = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"neon\",\n    instrument = \"KOSMOS\",\n    crop_bds = DATA_REGION,\n)\n\n# Loads target object exposures\nraw_science = ss.collect_images_array(\n    path = DATA_PATH,\n    tag = \"toi3884\",\n    instrument = \"KOSMOS\",\n    crop_bds = DATA_REGION,\n)\n</pre> CAL_PATH = \"../data/KOSMOS/calibrations\" DATA_PATH = \"../data/KOSMOS/target\" DATA_REGION = (175, 300)  # Loads standard calibration images bias = ss.average_matching_files(     path = CAL_PATH,     tag = \"bias\",     instrument = \"KOSMOS\",     crop_bds = DATA_REGION, ) raw_flat = ss.average_matching_files(     path = CAL_PATH,     tag = \"flat\",      instrument=\"KOSMOS\",     crop_bds = DATA_REGION, ) raw_arc = ss.average_matching_files(     path = CAL_PATH,     tag = \"neon\",     instrument = \"KOSMOS\",     crop_bds = DATA_REGION, )  # Loads target object exposures raw_science = ss.collect_images_array(     path = DATA_PATH,     tag = \"toi3884\",     instrument = \"KOSMOS\",     crop_bds = DATA_REGION, ) <p>Before we proceed with our reduction, it's worth pointing out two specific features...</p> In\u00a0[3]: Copied! <pre>ss.plot_image(\n    bias,\n    title = \"KOSMOS Bias Exposure\",\n)\n\nss.plot_image(\n    raw_flat,\n    norm = 'log',\n    title = \"KOSMOS Flatfield Exposure\",\n)\n</pre> ss.plot_image(     bias,     title = \"KOSMOS Bias Exposure\", )  ss.plot_image(     raw_flat,     norm = 'log',     title = \"KOSMOS Flatfield Exposure\", ) <p>Firstly, the bias exposure has a clear geometric pattern shown up across the detector. The APO technical documentation describes this as \"herringbone noise\" and is completely normal for the instrument, but should be mitigated as much as possible by averaging exposures where appropriate. Secondly, the flatfield (although hard to see) is saturated toward the right end of the detector. We mistakenly forgot to use a neutral density filter (ND5) to reduce the intensity of the internal quart lamp. Due to this inaccuracy, we will not perform a flatfield calibration in this example.</p> <p>Since we are dropping the flatfield exposure, we only need to subtract of the bias for our arclamp / science exposures...</p> In\u00a0[4]: Copied! <pre>arc = raw_arc - bias\nscience = raw_science - bias\n</pre> arc = raw_arc - bias science = raw_science - bias <p>Note</p> <p>         For many telescopes, you may also take a series of \"dark\" exposures to see how much light your instrument gives off when not illuminated. While this is typically good practice, dark current contamination is too low to significantly impact our reduction. APO's technical documentation estimates that dark current only contributes ~1 DN / minute.     </p> In\u00a0[5]: Copied! <pre>line_locations, rel_intensities = ss.find_cal_lines(\n    image = arc,\n    std_variation = 250,\n    debug = True,\n)\n</pre> line_locations, rel_intensities = ss.find_cal_lines(     image = arc,     std_variation = 250,     debug = True, ) <p>In the above plot, you should hopefully see a significant number of well-separated lines. If your threshold (set by <code>std_variations</code>) is set too low, then you risk accidentally lumping together many nearby features into one effective location. Once you satisfied with the above line positions, we feed them (along with our arclamp exposure) into...</p> In\u00a0[6]: Copied! <pre>warp_model = ss.generate_warp_model(\n    image = arc,\n    guess = line_locations,\n    debug = True,\n)\n</pre> warp_model = ss.generate_warp_model(     image = arc,     guess = line_locations,     debug = True, ) <p>In this plot, we are checking to make sure that the white grid does a decent job of following the shape of lines in that region. We can see how lines in the upper subplots are much more curved than lines in the lower subplots. Since the white grid seems to be accurately tracing out these lines, we can use this \"warp model\" to extract the background exposures using...</p> In\u00a0[7]: Copied! <pre>backgrounds = ss.extract_background(\n    images = science,\n    warp_model = warp_model,\n    mask_region = (40, 80),\n    debug = True,\n)\n</pre> backgrounds = ss.extract_background(     images = science,     warp_model = warp_model,     mask_region = (40, 80),     debug = True, ) <p>If the warp model is accurate and the extraction was a success, the \"corrected exposure\" should look like our signal trace with surrounding visual static. This random static is an indication that all geometry in our background exposures have been accurately removed!</p> In\u00a0[8]: Copied! <pre>boxcar_flux, boxcar_error = ss.boxcar_extraction(\n    images = science - backgrounds,\n    backgrounds = backgrounds,\n    RN = 6.0,\n    debug = True,\n)\n</pre> boxcar_flux, boxcar_error = ss.boxcar_extraction(     images = science - backgrounds,     backgrounds = backgrounds,     RN = 6.0,     debug = True, ) <p>Note how the extracted SED is pretty noisy! We can also see a fair number of points that lie far above the median SED (caused by cosmic rays). Both of these are somewhat mitigated by using a profile-weighted extraction...</p> In\u00a0[9]: Copied! <pre>horne_flux, horne_error = ss.horne_extraction(\n    images = science - backgrounds,\n    backgrounds = backgrounds,\n    RN = 6.0,\n    debug = True,\n)\n</pre> horne_flux, horne_error = ss.horne_extraction(     images = science - backgrounds,     backgrounds = backgrounds,     RN = 6.0,     debug = True, ) <p>This looks a lot cleaner! However, one major drawback is that it runs much slower that the boxcar extraction. In general, the boxcar extraction is useful for getting a quick look at your data, and the profile-weighted extraction is best for high-quality reductions. One easy way to compare the quality of both reduction methods is to look that the SNR of the extracted spectra...</p> In\u00a0[10]: Copied! <pre># Plots Boxcar Extraction\nss.plot_image(\n    boxcar_flux/boxcar_error,\n    title = \"Boxcar Extraction SNR\",\n    xlabel = \"Pixel Position (Dispersion Axis)\",\n    ylabel = \"Exposure Index\",\n    vmin = 0,\n    vmax = 150,\n)\n\n# Plots Horne Extraction\nss.plot_image(\n    horne_flux/horne_error,\n    title = \"Horne Extraction SNR\",\n    xlabel=\"Pixel Position (Dispersion Axis)\",\n    ylabel=\"Exposure Index\",\n    vmin = 0,\n    vmax = 150,\n)\n</pre> # Plots Boxcar Extraction ss.plot_image(     boxcar_flux/boxcar_error,     title = \"Boxcar Extraction SNR\",     xlabel = \"Pixel Position (Dispersion Axis)\",     ylabel = \"Exposure Index\",     vmin = 0,     vmax = 150, )  # Plots Horne Extraction ss.plot_image(     horne_flux/horne_error,     title = \"Horne Extraction SNR\",     xlabel=\"Pixel Position (Dispersion Axis)\",     ylabel=\"Exposure Index\",     vmin = 0,     vmax = 150, ) <p>Note how the boxcar extraction produces a much lower SNR than the Horne extraction (particularly at faint wavelengths)!</p> In\u00a0[11]: Copied! <pre># Fill with your own data\npixel_list = [...]\nwavelength_list = [...]\n\n# Initialize widget\nwavecal_widget = ss.WavecalWidget(\n    upper_lines = pixel_list,\n    lower_lines = wavelength_list,\n)\n\n#wavecal_widget.activate_widget()   &lt;--- Uncomment this to use widget\n</pre> # Fill with your own data pixel_list = [...] wavelength_list = [...]  # Initialize widget wavecal_widget = ss.WavecalWidget(     upper_lines = pixel_list,     lower_lines = wavelength_list, )  #wavecal_widget.activate_widget()   &lt;--- Uncomment this to use widget <p></p>"},{"location":"kosmos_example/#what-is-kosmos","title":"What is KOSMOS?\u00b6","text":"<p>Attached to the 3.5-meter telescope at Apache Point Observatory is the Kitt Peak Ohio State Multi-Object Spectrograph (KOSMOS). This is a long-slit multi-object spectograph capable of observing from blue-visible into near-infrared wavelengths of light with a resolving power of approximately 2600. Since this is a ground-based telescope, it is subject to the time-dependent contamination of Earth's atmosphere.</p> <p>In the following reduction, we process five exposures of TOI-3884 using KOSMOS' red grism (~5600-9400 Anstroms). We will not perform a wavelength calibration (since this relies on interactive code), and we will not perform a flux calibration (in an attempt to keep as little data as possible on the GitHub repository).</p>"},{"location":"kosmos_example/#reduction","title":"Reduction\u00b6","text":""},{"location":"kosmos_example/#loading-data","title":"Loading Data\u00b6","text":"<p>Data taken from KOSMOS is relatively simple to process. It has a single gain value shared across all pixels (stored in metadata as \"GAIN\") and does not split up its two chips into separate portions of the FITS file. This means we can use the same function calls showed in the preceeding documentation!</p>"},{"location":"kosmos_example/#background-extraction","title":"Background Extraction\u00b6","text":"<p>Next, we will develop a \"warp model\" that describes how light is curved onto our detector. To do this, you will need an arclamp exposure and a list of pixel locations where you have identified strong line emissions. You can retrieve these line positions using...</p>"},{"location":"kosmos_example/#flux-extraction","title":"Flux Extraction\u00b6","text":"<p>There are currently two method available for extracting flux from your science exposures. These correspond with the standard \"boxcar\" and \"horne (optimal)\" extractions used in many other reduction pipelines. Since the boxcar extraction is less likely to hit a snag and fail, let's start there...</p>"},{"location":"kosmos_example/#what-comes-next","title":"What Comes Next?\u00b6","text":"<p>Once you have an extracted SED, your next steps will depend greatly on the purpose of your observation. If you care about the flux-calibrated SED of your target, then you will need to use some of the functions outlined in <code>Additional Tools &gt; Flux Calibration</code>. If you are only interested in the relative changes in brightness for your target, then you may need to repeat the above reduction for another in-slit comparison star.</p> <p>Regardless, you will likely need to perform a wavelength calibration to convert pixel position into units of wavelength. Consider using our interactive widget for this!</p>"},{"location":"loading/","title":"Loading Data","text":"<p>The Flexible Image Transport System (FITS) is the standard filetype used for storing data taken from a telescope. There are several Python packages that exist to help load FITS files into a Python data structure. However, the structure of a FITS file can vary significantly between telescopes, making it difficult to automate data preparation. Since we know what type of data we are processing, we can wrap a standard FITS loader with some additional logic to make this initial step slightly easier!</p> <p>In this section, we show how <code>specsuite</code> can load / average several FITS files simultaneously. Unless you are processing data from one of the \"supported\" instruments (SBO, KOSMOS, or GMOS-N), you may also need to provide some additional information to help properly load your data.</p> <p>Note</p> <p>          We are working on creating specific loading functions for commonly-used telescopes with long-slit spectrographs. Although we have a \"default\" loader, this can fail to account for the niche data layout of some telescopes. We have dedicated loading functions for: KOSMOS, GMOS-N.     </p> <p>Warning</p> <p>          Currently, the following functions can only load FITS files. Some observatories will save data in a compressed format (such as GZIP compressed files). If you would like to use these loading functions, please unzip your data beforehand.     </p> In\u00a0[1]: Copied! <pre>import specsuite as ss\n\n# Specifies where to look for files / how to load them\nFILEPATH = \"../data/KOSMOS/target\"\nTAG = \"toi3884\"\nINSTRUMENT = \"kosmos\"\n\n# Loads an array of data images\ndata_images = ss.collect_images_array(\n    path = FILEPATH,\n    tag = TAG,\n    instrument = INSTRUMENT,\n    debug = True,\n)\n\n# Plots the first data image\nss.plot_image(data_images[0], norm='log', vmin=3e3, vmax=1e4)\n</pre> import specsuite as ss  # Specifies where to look for files / how to load them FILEPATH = \"../data/KOSMOS/target\" TAG = \"toi3884\" INSTRUMENT = \"kosmos\"  # Loads an array of data images data_images = ss.collect_images_array(     path = FILEPATH,     tag = TAG,     instrument = INSTRUMENT,     debug = True, )  # Plots the first data image ss.plot_image(data_images[0], norm='log', vmin=3e3, vmax=1e4) <pre>\nSearching for files with 'toi3884' tag...\n------------------------------------------\ntoi3884.0033.fits\ntoi3884.0034.fits\ntoi3884.0035.fits\ntoi3884.0036.fits\ntoi3884.0037.fits\n</pre> <p>If a file fails to load, it will simply be skipped during the loading process. You can check which specific files failed to load using the <code>debug</code> argument. Another optional argument can be provided to specify which region of an image to keep...</p> In\u00a0[2]: Copied! <pre># Specifies which rows to keep from an image\nDATA_REGION = (700, 800)\n\n# Loads an array of data images\ndata_images = ss.collect_images_array(\n    path = FILEPATH,\n    tag = TAG,\n    instrument = INSTRUMENT,\n    crop_bds = DATA_REGION,\n)\n\n# Plots the first data exposure\nss.plot_image(data_images[0], norm='log', vmin=3e3, vmax=1e4)\n</pre> # Specifies which rows to keep from an image DATA_REGION = (700, 800)  # Loads an array of data images data_images = ss.collect_images_array(     path = FILEPATH,     tag = TAG,     instrument = INSTRUMENT,     crop_bds = DATA_REGION, )  # Plots the first data exposure ss.plot_image(data_images[0], norm='log', vmin=3e3, vmax=1e4) <p>You may also want to exclude specific files in your directory. To skip specific filenames, use the <code>ignore</code> argument...</p> In\u00a0[3]: Copied! <pre># Name of the file in PATH to avoid loading\nIGNORE = [\"toi3884.0035.fits\"]\n\n# Loads all but one of the files PATH\ndata_images = ss.collect_images_array(\n    path = FILEPATH,\n    tag = TAG,\n    ignore = IGNORE,\n    instrument = INSTRUMENT,\n    debug = True,\n)\n</pre> # Name of the file in PATH to avoid loading IGNORE = [\"toi3884.0035.fits\"]  # Loads all but one of the files PATH data_images = ss.collect_images_array(     path = FILEPATH,     tag = TAG,     ignore = IGNORE,     instrument = INSTRUMENT,     debug = True, ) <pre>\nSearching for files with 'toi3884' tag...\n------------------------------------------\ntoi3884.0033.fits\ntoi3884.0034.fits\ntoi3884.0036.fits\ntoi3884.0037.fits\n</pre> <p>A common practice for calibration images (i.e., biases, darks, flats, etc.) is the take the average of multiple exposures. This is done to minimize the impact of random, statistical fluctuations on your data calibration. To load the average of several files, you can use <code>average_matching_files()</code> with the same arguments used above...</p> In\u00a0[4]: Copied! <pre>FILEPATH = \"../data/KOSMOS/calibrations\"\n\nbias = ss.average_matching_files(\n    path = FILEPATH,\n    tag = \"bias\",\n    instrument = INSTRUMENT,\n)\n\narclamp = ss.average_matching_files(\n    path = FILEPATH,\n    tag = \"neon\",\n    instrument = INSTRUMENT,\n)\n\nflat = ss.average_matching_files(\n    path = FILEPATH,\n    tag = \"flat\",\n    instrument = INSTRUMENT,\n)\n</pre> FILEPATH = \"../data/KOSMOS/calibrations\"  bias = ss.average_matching_files(     path = FILEPATH,     tag = \"bias\",     instrument = INSTRUMENT, )  arclamp = ss.average_matching_files(     path = FILEPATH,     tag = \"neon\",     instrument = INSTRUMENT, )  flat = ss.average_matching_files(     path = FILEPATH,     tag = \"flat\",     instrument = INSTRUMENT, ) <p>Note that you can specify a sub-region to keep just like in <code>collect_images_array()</code>.</p> In\u00a0[5]: Copied! <pre>FILEPATH = \"../data/KOSMOS/target\"\n\nmetadata = ss.load_metadata(\n    path = FILEPATH,\n    tag = \"toi3884\",\n)\n</pre> FILEPATH = \"../data/KOSMOS/target\"  metadata = ss.load_metadata(     path = FILEPATH,     tag = \"toi3884\", ) <p>This function can compile the metadata from multiple images into a single dictionary. If the value is identical between all images, then the dictionary value will just contain a single value...</p> In\u00a0[6]: Copied! <pre>print(\"Exposure Time:\", metadata[\"EXPTIME\"])\nprint(\"Instrument Name:\", metadata[\"INSTRUME\"])\n</pre> print(\"Exposure Time:\", metadata[\"EXPTIME\"]) print(\"Instrument Name:\", metadata[\"INSTRUME\"]) <pre>Exposure Time: 66.0\nInstrument Name: kosmos\n</pre> <p>If each exposure contains unique data, though, the the metadata dictionary will contain a list of all values. The order of each entry should align with the order that files were loaded in...</p> In\u00a0[7]: Copied! <pre>print(\"Airmass:\", metadata[\"AIRMASS\"])\n</pre> print(\"Airmass:\", metadata[\"AIRMASS\"]) <pre>Airmass: [2.12510104 2.10341395 2.08106647 2.06040547 2.04021965]\n</pre> In\u00a0[8]: Copied! <pre>PATH = \"../data/KOSMOS/calibrations\"\n\nbad_image = ss.average_matching_files(\n    path = PATH,\n    tag = \"a\",\n    debug = True,\n)\n</pre> PATH = \"../data/KOSMOS/calibrations\"  bad_image = ss.average_matching_files(     path = PATH,     tag = \"a\",     debug = True, ) <pre>\nSearching for files with 'a' tag...\n------------------------------------------\nbias.0001.fits\nbias.0002.fits\nbias.0003.fits\nbias.0004.fits\nbias.0005.fits\nflat.0029.fits\nflat.0030.fits\nflat.0031.fits\nflat.0032.fits\nflat.0033.fits\nfluxcal.0215.fits\n</pre> <pre>\nImage statistics for average 'a' image...\n      Min: 3035.0\n      Max: 53835.0\n     Mean: 3771.595\n      STD: 422.12\n</pre> <p>If your files have unexpected features, using <code>debug = True</code> can help you quickly check which files are being loaded.</p> In\u00a0[9]: Copied! <pre>data = ss.average_matching_files(\n    path = \"../data/KOSMOS/calibrations\",\n    tag = \"bad tag\",\n    debug = True,\n)\n</pre> data = ss.average_matching_files(     path = \"../data/KOSMOS/calibrations\",     tag = \"bad tag\",     debug = True, ) <pre>\nSearching for files with 'bad tag' tag...\n------------------------------------------\n</pre> <pre>UserWarning: No images were successfully loaded, returning 'None' instead</pre> <p>The easiest way to fix this is to use the <code>debug</code> argument to verify whether the expected files are successfully loaded.</p>"},{"location":"loading/#basic-usage","title":"Basic Usage\u00b6","text":"<p>Across all documentation, we will be using the sample data location in the \"data/\" directory on the <code>specsuite</code> repository. Although we recommend splitting up your exopsures into separate \"calibration/\" and \"target/\" folders, this is not strictly necessary for our loading functions to work. For reference, here is the structure of our data...</p> <pre>\ud83d\udcc2KOSMOS\n|\n\u2514\u2500\u2500\ud83d\udcc2calibrations\n\u2502  \u2502   \ud83d\udcdcbias.0001.fits\n\u2502  \u2502   \ud83d\udcdcbias.0002.fits\n\u2502  \u2502   \ud83d\udcdcbias.0003.fits\n\u2502  \u2502   \ud83d\udcdcflatfield.0004.fits\n\u2502  \u2502   \ud83d\udcdcflatfield.0005.fits\n\u2502  \u2502          \u22ee\n\u2502   \n\u2514\u2500\u2500\ud83d\udcc2target\n   \u2502   \ud83d\udcdctoi3884.0006.fits\n   \u2502   \ud83d\udcdctoi3884.0007.fits\n   \u2502   \ud83d\udcdctoi3884.0008.fits\n   \u2502   \ud83d\udcdctoi3884.0009.fits\n   \u2502   \ud83d\udcdctoi3884.0010.fits\n   \u2502          \u22ee\n</pre>"},{"location":"loading/#loading-multiple-images","title":"Loading Multiple Images\u00b6","text":"<p>There are many types of exposures you may wish to load into Python as separate Numpy arrays. Using <code>collect_images_array()</code>, you can do just that! This function requires a directory to search for FITS files and a 'tag' to look for in the filenames contained within that directory. So, if we wanted to separately load in each \"toi3884\" exposure from our data directory...</p>"},{"location":"loading/#averaging-multiple-images","title":"Averaging Multiple Images\u00b6","text":""},{"location":"loading/#compiling-fits-metadata","title":"Compiling FITS Metadata\u00b6","text":"<p>FITS headers often contain useful metadata (i.e., exposure time, telescope name, airmass, etc.). While we cannot predict which exact keywords are used in a given file, we create a dictionary filled with all of the key-value pairs found within the FITS header. This is as simple as providing a path and tag...</p>"},{"location":"loading/#common-errors","title":"Common Errors\u00b6","text":""},{"location":"loading/#wrong-files-loading","title":"Wrong Files Loading\u00b6","text":"<p>If you happen to have multiple types of exposures in a single directory, your provided tag matters a lot. For example, if my provided tag was \"a\", <code>specsuite</code> may load two different types of files together...</p>"},{"location":"loading/#the-loaded-data-is-none","title":"The Loaded Data is 'None'\u00b6","text":"<p>If <code>specsuite</code> fails to load a file, it will attempt to skip over the file and carry on until the next valid file is loaded. However, if all files fail to load (or no valid files were found in the provided directory) then the expected return is 'None'. For example...</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>To begin our data reduction, we need to load in several FITS files into Python-ready data structures. Using <code>specsuite</code>, you can quickly load your calibration and science exposures into Numpy arrays! Let's do this for some sample data located on the GitHub repository...</p> In\u00a0[1]: Copied! <pre>import specsuite as ss\n\n# Defines where to look for data\nDATA_REGION = (700, 800)\nCAL_PATH = \"../data/KOSMOS/calibrations\"\nDATA_PATH = \"../data/KOSMOS/target\"\nINSTRUMENT = \"kosmos\"\n\n# Loads average of all 'bias' exposures\nbias = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"bias\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT,\n)\n\n# Loads average of all 'flat' exposures\nflat = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"flat\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT,\n) - bias\n\n# Loads average of all 'neon' exposures\narclamp = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"neon\",\n    crop_bds = DATA_REGION,\n    instrument = INSTRUMENT,\n) - bias\n\n# Loads all 'toi3884' exposures as separate arrays\nscience = ss.collect_images_array(\n    DATA_PATH,\n    \"toi3884\",\n    crop_bds=DATA_REGION,\n    instrument = INSTRUMENT,\n) - bias\n</pre> import specsuite as ss  # Defines where to look for data DATA_REGION = (700, 800) CAL_PATH = \"../data/KOSMOS/calibrations\" DATA_PATH = \"../data/KOSMOS/target\" INSTRUMENT = \"kosmos\"  # Loads average of all 'bias' exposures bias = ss.average_matching_files(     path = CAL_PATH,     tag = \"bias\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT, )  # Loads average of all 'flat' exposures flat = ss.average_matching_files(     path = CAL_PATH,     tag = \"flat\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT, ) - bias  # Loads average of all 'neon' exposures arclamp = ss.average_matching_files(     path = CAL_PATH,     tag = \"neon\",     crop_bds = DATA_REGION,     instrument = INSTRUMENT, ) - bias  # Loads all 'toi3884' exposures as separate arrays science = ss.collect_images_array(     DATA_PATH,     \"toi3884\",     crop_bds=DATA_REGION,     instrument = INSTRUMENT, ) - bias <p>We can use our flatfield exposure to perform a standard calibration on our data!</p> In\u00a0[2]: Copied! <pre>science = ss.flatfield_correction(\n    image = science,\n    flat = flat,\n    debug = True,\n)\n</pre> science = ss.flatfield_correction(     image = science,     flat = flat,     debug = True, ) <p>Using <code>specsuite</code>'s \"warp models,\" we can come up with an approximate model for how light is warped on the detector.</p> In\u00a0[3]: Copied! <pre># Identifies the pixel positions of line emissions\nlocs, _ = ss.find_cal_lines(\n    image = arclamp,\n    std_variation = 200,\n    debug = True,\n)\n\n# Models how line emissions are 'bent'\nwarp_model = ss.generate_warp_model(\n    image = arclamp,\n    guess = locs,\n    debug = True,\n)\n</pre> # Identifies the pixel positions of line emissions locs, _ = ss.find_cal_lines(     image = arclamp,     std_variation = 200,     debug = True, )  # Models how line emissions are 'bent' warp_model = ss.generate_warp_model(     image = arclamp,     guess = locs,     debug = True, ) <p>This \"warp model\" can then be used to perform a high-quality background extraction!</p> In\u00a0[4]: Copied! <pre>backgrounds = ss.extract_background(\n    images = science,\n    warp_model = warp_model,\n    mask_region = (40, 80),\n    debug = True,\n)\ncalibrated_science = science - backgrounds\n</pre> backgrounds = ss.extract_background(     images = science,     warp_model = warp_model,     mask_region = (40, 80),     debug = True, ) calibrated_science = science - backgrounds <p>While there are several functions available for performing a flux extraction, we can use a simple boxcar extraction to get a quick look at our data.</p> In\u00a0[5]: Copied! <pre>flux, error = ss.boxcar_extraction(\n    images = calibrated_science,\n    backgrounds = backgrounds,\n    RN = 6.0,\n    debug = True,\n)\n</pre> flux, error = ss.boxcar_extraction(     images = calibrated_science,     backgrounds = backgrounds,     RN = 6.0,     debug = True, ) <p>There is more <code>specsuite</code> has to offer, please take a look through the documentation to get an in-depth explanation of each of these steps (+ several additional tools)!</p>"},{"location":"wavelength_calibration/","title":"Wavelength Calibration","text":"<p>Up until this point in the reduction, we have not been able to describe any of our data in terms of wavelength. In order to convert pixel position along the dispersion axis to wavelength, we need to identify known emission lines in an arclamp image.</p> <p>In this section, we discuss how <code>specsuite</code> can be used to interactively match two lists of lines.</p> In\u00a0[1]: hide_code_block Copied! <pre>import specsuite as ss\nimport astropy.units as u\n\n# Defines where to look for data\nDATA_REGION = (700, 800)\nCAL_PATH = \"../data/KOSMOS/calibrations\"\nINSTRUMENT = \"kosmos\"\nWAV_BOUNDS = (550 * u.nm, 950 * u.nm)\n\n# Collects all the necessary exposures\nbias = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"bias\",\n    crop_bds = DATA_REGION,\n)\n\narclamp = ss.average_matching_files(\n    path = CAL_PATH,\n    tag = \"neon\",\n    crop_bds = DATA_REGION,\n) - bias\n\narclamp_lines, _ = ss.find_cal_lines(arclamp, std_variation=250)\n\n# A list of known emission features for APO's KOSMOS instrument (neon)\nknown_lines = [\n    5852.49, 5881.90, 5944.83, 6030.00, 6096.16, 6143.06, 6266.50, 6334.43, 6382.99, 6402.25,\n    6506.53, 6598.95, 6678.28, 6717.04, 6929.47, 6032.41, 7173.94, 7245.17, 7438.90, 7488.87,\n    8300.36, 8377.61, 8495.36, 8654.38, 8780.62, 8853.87,\n]\n</pre> import specsuite as ss import astropy.units as u  # Defines where to look for data DATA_REGION = (700, 800) CAL_PATH = \"../data/KOSMOS/calibrations\" INSTRUMENT = \"kosmos\" WAV_BOUNDS = (550 * u.nm, 950 * u.nm)  # Collects all the necessary exposures bias = ss.average_matching_files(     path = CAL_PATH,     tag = \"bias\",     crop_bds = DATA_REGION, )  arclamp = ss.average_matching_files(     path = CAL_PATH,     tag = \"neon\",     crop_bds = DATA_REGION, ) - bias  arclamp_lines, _ = ss.find_cal_lines(arclamp, std_variation=250)  # A list of known emission features for APO's KOSMOS instrument (neon) known_lines = [     5852.49, 5881.90, 5944.83, 6030.00, 6096.16, 6143.06, 6266.50, 6334.43, 6382.99, 6402.25,     6506.53, 6598.95, 6678.28, 6717.04, 6929.47, 6032.41, 7173.94, 7245.17, 7438.90, 7488.87,     8300.36, 8377.61, 8495.36, 8654.38, 8780.62, 8853.87, ]  In\u00a0[2]: Copied! <pre>wavecal_widget = ss.WavecalWidget(\n    upper_lines = known_lines,\n    lower_lines = arclamp_lines,\n)\n\n#wavecal_widget.activate_widget()   &lt;--- Uncomment this to use widget\n</pre> wavecal_widget = ss.WavecalWidget(     upper_lines = known_lines,     lower_lines = arclamp_lines, )  #wavecal_widget.activate_widget()   &lt;--- Uncomment this to use widget <p></p> <p>Warning</p> <p>         Behind the scenes, the widget tried to automatically change the Jupyter notebook backend back and forth from \"widget\" and \"inline\". If you notice weird behavior in your plots after using the widget, it is possible that this conversion did not work as intended. You can fix this by including %matplotlib inline at the top of the next cell.     </p> <p>There is a lot to unpack, so let's start by describing the first two plots. Here, you are seeing a visualization of the line lists you provided. The top plot corresponds to <code>known_lines</code>, and the second plot corresponds to <code>arclamp_lines</code>. These images are made by summing several Gaussian distributions with a mean at the line location and a shared standard deviation (we will come back to this). Hopefully you can see some shared pattern in both images, but it can be hard to discern exactly which lines to pair with one another. To help with matching, there are a handful of controls that allow you to modify the widget...</p> <p></p> <p>Let's briefly walk through what each of these buttons do. In the leftmost box...</p> <ul> <li>Reset Buttons: Resets all changes made to the corresponding line plot. All removed lines will be restored, and all matched pairs will be lost.</li> <li>Size Sliders: Adjusts the size of lines (standard deviation) in the corresponding line plot.</li> </ul> <p>In the middle box...</p> <ul> <li>Toggle Intensity: Controls whether line intensities are used to alter the brightness of the two line plots. If one (or both) line intensity lists were not provided, the related plot will not be affected by this button.</li> <li>Toggle Matched Fit: Controls whether to display the wavelength calibration for matched pairs will be displayed.</li> <li>Undo: Will revert the last action taken (including matching and removing lines). If pressed multiple times in a row, multiple actions will be undone.</li> <li>Residual Cap: The upper and lower bounds on the residual plot (this is the bottom plot).</li> <li>Fit Order: The order of polynomial used to fit to matched lines. Note that a valid fit will only be plotted if the number of matched lines is greater than the order of the polynomial plus one.</li> </ul> <p>In the rightmost box...</p> <ul> <li>Remove/Match Button: Determines the behavior of the widget when the upper two plots are clicked. When on \"Remove,\" the nearest line will be removed. When on \"Match,\" any pair of two lines from the first two plots will be grouped together.</li> <li>Close Widget Button: Terminates the widget's interactive behavior.</li> </ul> <p>The best way to familiarize yourself with these controls is to just try it for yourself! However, we can take a look at an example of the widget once a few lines have been \"matched up\"...</p> <p></p> <p>Note how dashed green lines show which lines have been paired, and the bottom three plots are now filled with some useful diagnostic information! In order, these plots show...</p> <ul> <li>The current polynomial fit based on your matched lines. Each point represents a single pair of lines where the x-value corresponds with <code>lower_lines</code> and the y-value corresponds with <code>upper_lines</code>.</li> <li>The derivative of the polynomial fit. Points are linearly interpolated using all matched lines.</li> <li>Residuals between the polynomial fit and all matched lines. The vertical bounds of this plot are controlled by the \"Residual Cap\" slider.</li> </ul> <p>These plots can help you determine how accurate your current calibration is looking. For example, if one point had an incredibly large residual, that is a potential indication that you made an error. Mistakes can be rectified either by using the \"Undo\" button (best for recent mistakes) or either of the \"Reset\" Buttons (for more severe / older mistakes).</p> <p>Once you have finished matching up lines, you can extract a list of these pairs and the final wavelength solution using the following code...</p> In\u00a0[3]: Copied! <pre>paired_lines = wavecal_widget.final_lines # List of all paired lines\np_wavecal = wavecal_widget.p # The final np.polynomial\npdr_wavecal = wavecal_widget.pdr # The final derivative of np.polynomial\n</pre> paired_lines = wavecal_widget.final_lines # List of all paired lines p_wavecal = wavecal_widget.p # The final np.polynomial pdr_wavecal = wavecal_widget.pdr # The final derivative of np.polynomial <p>Note</p> <p>         Currently, this is the only part of our reduction pipeline that cannot be automated. However, unless you need to perform additional wavelength-dependent corrections (such as those shown in \"Advanced Tools\"), this step is not necessary for getting a quick look at your data. We are currently working on an automated routine, but it requires more work and testing before an official release.     </p> In\u00a0[4]: Copied! <pre>%matplotlib widget\n\nwavecal_widget = ss.WavecalWidget(known_lines, arclamp_lines)\n#wavecal_widget.activate_widget()   &lt;----- Uncomment this to initialize the widget\n</pre> %matplotlib widget  wavecal_widget = ss.WavecalWidget(known_lines, arclamp_lines) #wavecal_widget.activate_widget()   &lt;----- Uncomment this to initialize the widget <p>Warning</p> <p>         We have found that the widget does not work properly when run on remote servers. This is related to complications in how Jupyter notebooks handle interactivity. If possible, please run this widget locally on your own device.     </p>"},{"location":"wavelength_calibration/#basic-usage","title":"Basic Usage\u00b6","text":""},{"location":"wavelength_calibration/#the-wavcal-widget","title":"The \"Wavcal Widget\"\u00b6","text":"<p>Since MKDocs can only create static websites, we cannot demonstrate <code>specsuite</code>'s interactive wavelength calibration widget here. We will provide the code necessary for running the widget and provide a series of static images to go along with them. To activate the widget, you need to create an instance of <code>WavecalWidget</code> with two lists of lines and call the <code>.activate_widget()</code> function...</p>"},{"location":"wavelength_calibration/#common-errors","title":"Common Errors\u00b6","text":""},{"location":"wavelength_calibration/#wavecalwidget-is-unresponsive","title":"WavecalWidget is unresponsive\u00b6","text":"<p>Under the hood, <code>WavecalWidget()</code> is trying to use a \"magic command\" to convert the Jupyeter notebook into an environment that can handle widgets. Specifically, it is attempting to run the equivalent of <code>%matplotlib widget</code>, then converting back to <code>%matplotlib inline</code> once the widget is closed. These only work in Jupyter-like environments, so any attempt to run this widget will result in an error. If your widget is loading, but is not interactive, try running <code>%matplotlib widget</code> at the top of your cell.</p>"},{"location":"wavelength_calibration/#no-lines-being-shown","title":"No Lines Being Shown\u00b6","text":"<p>Sometimes, the default width of lines is too small. This is caused by the way we are generating the \"simulated images\" since the standard deviation may be smaller than the width of a pixel. If you run into this issue, try adjusting the \"Size Sliders\" to a larger value.</p> <p>If that does not resolve this issue, the issue may be caused if you provide arguments for the line intensities. Check if toggling the \"Toggle Intensity\" button allows you to see lines. If that is the cause, make sure that your intensities are reasonable values.</p>"}]}